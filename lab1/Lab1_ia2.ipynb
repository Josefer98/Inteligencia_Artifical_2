{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Laboratorio 1 IA 2\n",
        "Nombres:José Fernando\n",
        "Apellidos:Alfaro Ayzama\n",
        "\n",
        "1.-Aplicar Pytorch para construir un MLP que permita clasificar almenos 5 clases(imagenes,cualquier cosa).\n",
        "\n",
        "2.-El mlp debe contener almenos unas 50 unidades neuronales en su capa oculta.\n",
        "\n",
        "3.-se debe entrenar con almenos 1000 epochs()esplicar que pasa\n",
        "\n",
        "4.-Guardar checkpoints de entrenamiento cada 20 epochs\n",
        "\n",
        "5.-utilizar los distintos checkpoints para reflexionar sobre los resultados(saber los resultados).\n",
        "\n",
        "6.-Aplicar las formas de exportacion de modelos.\n",
        "\n",
        "7.-Caracteristicas del datased(entradas n.X>=10 m>=10000 Y<=5)\n",
        "\n",
        "8.-Se debe coordinar entre estudiantes para evitar que dos o mas estudiantes utilizen el mismo dataset.\n",
        "\n",
        "9.-Utilizar de manera obligatoria objetos dataset y dataloader en la implementacion.\n",
        "\n",
        "Dataset Utilizado en este cuadernillo:\n",
        "https://www.kaggle.com/datasets/nimapourmoradi/dry-bean-dataset-classification\n",
        "\n",
        "link de github: repocitorio:https://github.com/Josefer98/Inteligencia_Artifical_2"
      ],
      "metadata": {
        "id": "FB5hhCXOUK4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import os\n",
        "\n",
        "# Cargar el conjunto de datos\n",
        "data = pd.read_csv(\"Dry_Bean_Dataset.csv\")\n",
        "\n",
        "# Preprocesamiento de datos\n",
        "label_encoder = LabelEncoder()\n",
        "data['Class'] = label_encoder.fit_transform(data['Class'])\n",
        "\n",
        "X = data.drop('Class', axis=1).values\n",
        "y = data['Class'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "class BeanDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = BeanDataset(X_train, y_train)\n",
        "test_dataset = BeanDataset(X_test, y_test)\n",
        "\n",
        "# Definir el modelo MLP\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 50\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "model = MLP(input_size, hidden_size, num_classes)\n",
        "\n",
        "# Definir la función de pérdida y el optimizador\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "num_epochs = 1000\n",
        "checkpoint_interval = 20\n",
        "checkpoint_dir = 'checkpoints'\n",
        "\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in DataLoader(train_dataset, batch_size=32, shuffle=True):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
        "\n",
        "    # Guardar puntos de control\n",
        "    if (epoch + 1) % checkpoint_interval == 0:\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pt')\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': epoch_loss\n",
        "        }, checkpoint_path)\n",
        "\n",
        "# Evaluación del modelo\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in DataLoader(test_dataset, batch_size=32):\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Accuracy on test set: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5PPypQWtU54",
        "outputId": "b98e3933-87ee-4b63-a8e1-d70c65e3bd30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1000], Loss: 276.0800\n",
            "Epoch [2/1000], Loss: 76.0071\n",
            "Epoch [3/1000], Loss: 73.2413\n",
            "Epoch [4/1000], Loss: 46.1005\n",
            "Epoch [5/1000], Loss: 50.5538\n",
            "Epoch [6/1000], Loss: 40.5256\n",
            "Epoch [7/1000], Loss: 30.3363\n",
            "Epoch [8/1000], Loss: 34.4130\n",
            "Epoch [9/1000], Loss: 42.2705\n",
            "Epoch [10/1000], Loss: 24.7135\n",
            "Epoch [11/1000], Loss: 23.8361\n",
            "Epoch [12/1000], Loss: 28.9978\n",
            "Epoch [13/1000], Loss: 23.9330\n",
            "Epoch [14/1000], Loss: 24.3575\n",
            "Epoch [15/1000], Loss: 21.2759\n",
            "Epoch [16/1000], Loss: 22.2859\n",
            "Epoch [17/1000], Loss: 19.3822\n",
            "Epoch [18/1000], Loss: 23.7937\n",
            "Epoch [19/1000], Loss: 17.8766\n",
            "Epoch [20/1000], Loss: 25.4990\n",
            "Epoch [21/1000], Loss: 14.0575\n",
            "Epoch [22/1000], Loss: 16.7596\n",
            "Epoch [23/1000], Loss: 17.5811\n",
            "Epoch [24/1000], Loss: 16.8116\n",
            "Epoch [25/1000], Loss: 12.4236\n",
            "Epoch [26/1000], Loss: 16.4822\n",
            "Epoch [27/1000], Loss: 15.7990\n",
            "Epoch [28/1000], Loss: 14.1537\n",
            "Epoch [29/1000], Loss: 14.6261\n",
            "Epoch [30/1000], Loss: 15.0612\n",
            "Epoch [31/1000], Loss: 15.9890\n",
            "Epoch [32/1000], Loss: 12.3926\n",
            "Epoch [33/1000], Loss: 13.2826\n",
            "Epoch [34/1000], Loss: 12.9483\n",
            "Epoch [35/1000], Loss: 17.0781\n",
            "Epoch [36/1000], Loss: 13.2716\n",
            "Epoch [37/1000], Loss: 11.2059\n",
            "Epoch [38/1000], Loss: 11.8439\n",
            "Epoch [39/1000], Loss: 13.8600\n",
            "Epoch [40/1000], Loss: 10.4805\n",
            "Epoch [41/1000], Loss: 8.5624\n",
            "Epoch [42/1000], Loss: 13.3154\n",
            "Epoch [43/1000], Loss: 10.7125\n",
            "Epoch [44/1000], Loss: 10.9710\n",
            "Epoch [45/1000], Loss: 8.2805\n",
            "Epoch [46/1000], Loss: 7.6779\n",
            "Epoch [47/1000], Loss: 10.3163\n",
            "Epoch [48/1000], Loss: 12.0463\n",
            "Epoch [49/1000], Loss: 7.7044\n",
            "Epoch [50/1000], Loss: 9.9010\n",
            "Epoch [51/1000], Loss: 8.7054\n",
            "Epoch [52/1000], Loss: 7.4830\n",
            "Epoch [53/1000], Loss: 8.4587\n",
            "Epoch [54/1000], Loss: 8.9275\n",
            "Epoch [55/1000], Loss: 8.2882\n",
            "Epoch [56/1000], Loss: 6.7948\n",
            "Epoch [57/1000], Loss: 8.7579\n",
            "Epoch [58/1000], Loss: 8.3743\n",
            "Epoch [59/1000], Loss: 7.1889\n",
            "Epoch [60/1000], Loss: 7.5740\n",
            "Epoch [61/1000], Loss: 7.7217\n",
            "Epoch [62/1000], Loss: 6.7384\n",
            "Epoch [63/1000], Loss: 6.3153\n",
            "Epoch [64/1000], Loss: 6.5392\n",
            "Epoch [65/1000], Loss: 5.6251\n",
            "Epoch [66/1000], Loss: 7.5803\n",
            "Epoch [67/1000], Loss: 5.8530\n",
            "Epoch [68/1000], Loss: 4.4329\n",
            "Epoch [69/1000], Loss: 6.0989\n",
            "Epoch [70/1000], Loss: 5.1577\n",
            "Epoch [71/1000], Loss: 4.4681\n",
            "Epoch [72/1000], Loss: 5.9132\n",
            "Epoch [73/1000], Loss: 5.0489\n",
            "Epoch [74/1000], Loss: 4.6699\n",
            "Epoch [75/1000], Loss: 4.9358\n",
            "Epoch [76/1000], Loss: 4.0267\n",
            "Epoch [77/1000], Loss: 5.3841\n",
            "Epoch [78/1000], Loss: 4.0365\n",
            "Epoch [79/1000], Loss: 3.7942\n",
            "Epoch [80/1000], Loss: 4.2488\n",
            "Epoch [81/1000], Loss: 4.0111\n",
            "Epoch [82/1000], Loss: 3.8139\n",
            "Epoch [83/1000], Loss: 3.1970\n",
            "Epoch [84/1000], Loss: 3.6341\n",
            "Epoch [85/1000], Loss: 4.6443\n",
            "Epoch [86/1000], Loss: 2.9582\n",
            "Epoch [87/1000], Loss: 3.2715\n",
            "Epoch [88/1000], Loss: 2.4434\n",
            "Epoch [89/1000], Loss: 2.6296\n",
            "Epoch [90/1000], Loss: 3.1689\n",
            "Epoch [91/1000], Loss: 3.0121\n",
            "Epoch [92/1000], Loss: 2.2860\n",
            "Epoch [93/1000], Loss: 3.0221\n",
            "Epoch [94/1000], Loss: 2.8357\n",
            "Epoch [95/1000], Loss: 2.4452\n",
            "Epoch [96/1000], Loss: 2.4709\n",
            "Epoch [97/1000], Loss: 2.0201\n",
            "Epoch [98/1000], Loss: 2.6604\n",
            "Epoch [99/1000], Loss: 1.8673\n",
            "Epoch [100/1000], Loss: 1.5232\n",
            "Epoch [101/1000], Loss: 1.3746\n",
            "Epoch [102/1000], Loss: 1.5591\n",
            "Epoch [103/1000], Loss: 1.7855\n",
            "Epoch [104/1000], Loss: 1.9679\n",
            "Epoch [105/1000], Loss: 1.8627\n",
            "Epoch [106/1000], Loss: 1.6856\n",
            "Epoch [107/1000], Loss: 1.2866\n",
            "Epoch [108/1000], Loss: 1.3061\n",
            "Epoch [109/1000], Loss: 1.4319\n",
            "Epoch [110/1000], Loss: 1.2045\n",
            "Epoch [111/1000], Loss: 2.2251\n",
            "Epoch [112/1000], Loss: 0.9128\n",
            "Epoch [113/1000], Loss: 1.2918\n",
            "Epoch [114/1000], Loss: 1.0107\n",
            "Epoch [115/1000], Loss: 1.0310\n",
            "Epoch [116/1000], Loss: 0.9152\n",
            "Epoch [117/1000], Loss: 0.9344\n",
            "Epoch [118/1000], Loss: 1.1909\n",
            "Epoch [119/1000], Loss: 1.0327\n",
            "Epoch [120/1000], Loss: 0.7205\n",
            "Epoch [121/1000], Loss: 0.8747\n",
            "Epoch [122/1000], Loss: 0.9409\n",
            "Epoch [123/1000], Loss: 0.6726\n",
            "Epoch [124/1000], Loss: 0.7868\n",
            "Epoch [125/1000], Loss: 0.9205\n",
            "Epoch [126/1000], Loss: 0.5207\n",
            "Epoch [127/1000], Loss: 0.9476\n",
            "Epoch [128/1000], Loss: 0.8912\n",
            "Epoch [129/1000], Loss: 0.8993\n",
            "Epoch [130/1000], Loss: 0.9887\n",
            "Epoch [131/1000], Loss: 0.7818\n",
            "Epoch [132/1000], Loss: 0.6039\n",
            "Epoch [133/1000], Loss: 0.5915\n",
            "Epoch [134/1000], Loss: 0.6845\n",
            "Epoch [135/1000], Loss: 0.5950\n",
            "Epoch [136/1000], Loss: 0.6202\n",
            "Epoch [137/1000], Loss: 0.5178\n",
            "Epoch [138/1000], Loss: 0.4842\n",
            "Epoch [139/1000], Loss: 0.5423\n",
            "Epoch [140/1000], Loss: 0.6472\n",
            "Epoch [141/1000], Loss: 0.5460\n",
            "Epoch [142/1000], Loss: 0.5104\n",
            "Epoch [143/1000], Loss: 0.4859\n",
            "Epoch [144/1000], Loss: 0.5524\n",
            "Epoch [145/1000], Loss: 0.5237\n",
            "Epoch [146/1000], Loss: 0.4860\n",
            "Epoch [147/1000], Loss: 0.5331\n",
            "Epoch [148/1000], Loss: 0.5328\n",
            "Epoch [149/1000], Loss: 0.5545\n",
            "Epoch [150/1000], Loss: 0.5064\n",
            "Epoch [151/1000], Loss: 0.5034\n",
            "Epoch [152/1000], Loss: 0.4981\n",
            "Epoch [153/1000], Loss: 0.5118\n",
            "Epoch [154/1000], Loss: 0.4553\n",
            "Epoch [155/1000], Loss: 0.5353\n",
            "Epoch [156/1000], Loss: 0.5103\n",
            "Epoch [157/1000], Loss: 0.4863\n",
            "Epoch [158/1000], Loss: 0.5291\n",
            "Epoch [159/1000], Loss: 0.4918\n",
            "Epoch [160/1000], Loss: 0.4806\n",
            "Epoch [161/1000], Loss: 0.4615\n",
            "Epoch [162/1000], Loss: 0.4696\n",
            "Epoch [163/1000], Loss: 0.5245\n",
            "Epoch [164/1000], Loss: 0.4497\n",
            "Epoch [165/1000], Loss: 0.4629\n",
            "Epoch [166/1000], Loss: 0.5115\n",
            "Epoch [167/1000], Loss: 0.4813\n",
            "Epoch [168/1000], Loss: 0.4711\n",
            "Epoch [169/1000], Loss: 0.4540\n",
            "Epoch [170/1000], Loss: 0.5064\n",
            "Epoch [171/1000], Loss: 0.4610\n",
            "Epoch [172/1000], Loss: 0.5112\n",
            "Epoch [173/1000], Loss: 0.4702\n",
            "Epoch [174/1000], Loss: 0.4712\n",
            "Epoch [175/1000], Loss: 0.4551\n",
            "Epoch [176/1000], Loss: 0.4911\n",
            "Epoch [177/1000], Loss: 0.4936\n",
            "Epoch [178/1000], Loss: 0.4879\n",
            "Epoch [179/1000], Loss: 0.4738\n",
            "Epoch [180/1000], Loss: 0.4807\n",
            "Epoch [181/1000], Loss: 0.4704\n",
            "Epoch [182/1000], Loss: 0.4437\n",
            "Epoch [183/1000], Loss: 0.4682\n",
            "Epoch [184/1000], Loss: 0.4365\n",
            "Epoch [185/1000], Loss: 0.4400\n",
            "Epoch [186/1000], Loss: 0.4432\n",
            "Epoch [187/1000], Loss: 0.4468\n",
            "Epoch [188/1000], Loss: 0.4704\n",
            "Epoch [189/1000], Loss: 0.4624\n",
            "Epoch [190/1000], Loss: 0.4857\n",
            "Epoch [191/1000], Loss: 0.4591\n",
            "Epoch [192/1000], Loss: 0.4575\n",
            "Epoch [193/1000], Loss: 0.4557\n",
            "Epoch [194/1000], Loss: 0.4844\n",
            "Epoch [195/1000], Loss: 0.4663\n",
            "Epoch [196/1000], Loss: 0.4678\n",
            "Epoch [197/1000], Loss: 0.4136\n",
            "Epoch [198/1000], Loss: 0.4398\n",
            "Epoch [199/1000], Loss: 0.4222\n",
            "Epoch [200/1000], Loss: 0.4861\n",
            "Epoch [201/1000], Loss: 0.4460\n",
            "Epoch [202/1000], Loss: 0.4438\n",
            "Epoch [203/1000], Loss: 0.4490\n",
            "Epoch [204/1000], Loss: 0.4403\n",
            "Epoch [205/1000], Loss: 0.4213\n",
            "Epoch [206/1000], Loss: 0.4935\n",
            "Epoch [207/1000], Loss: 0.4560\n",
            "Epoch [208/1000], Loss: 0.4695\n",
            "Epoch [209/1000], Loss: 0.5272\n",
            "Epoch [210/1000], Loss: 0.4023\n",
            "Epoch [211/1000], Loss: 0.4297\n",
            "Epoch [212/1000], Loss: 0.4514\n",
            "Epoch [213/1000], Loss: 0.4757\n",
            "Epoch [214/1000], Loss: 0.4296\n",
            "Epoch [215/1000], Loss: 0.4379\n",
            "Epoch [216/1000], Loss: 0.4632\n",
            "Epoch [217/1000], Loss: 0.4297\n",
            "Epoch [218/1000], Loss: 0.4166\n",
            "Epoch [219/1000], Loss: 0.4685\n",
            "Epoch [220/1000], Loss: 0.4344\n",
            "Epoch [221/1000], Loss: 0.4598\n",
            "Epoch [222/1000], Loss: 0.4254\n",
            "Epoch [223/1000], Loss: 0.4333\n",
            "Epoch [224/1000], Loss: 0.4105\n",
            "Epoch [225/1000], Loss: 0.4540\n",
            "Epoch [226/1000], Loss: 0.4538\n",
            "Epoch [227/1000], Loss: 0.4548\n",
            "Epoch [228/1000], Loss: 0.4232\n",
            "Epoch [229/1000], Loss: 0.4354\n",
            "Epoch [230/1000], Loss: 0.4244\n",
            "Epoch [231/1000], Loss: 0.4792\n",
            "Epoch [232/1000], Loss: 0.4415\n",
            "Epoch [233/1000], Loss: 0.4400\n",
            "Epoch [234/1000], Loss: 0.4172\n",
            "Epoch [235/1000], Loss: 0.4259\n",
            "Epoch [236/1000], Loss: 0.4401\n",
            "Epoch [237/1000], Loss: 0.4133\n",
            "Epoch [238/1000], Loss: 0.4294\n",
            "Epoch [239/1000], Loss: 0.4459\n",
            "Epoch [240/1000], Loss: 0.4196\n",
            "Epoch [241/1000], Loss: 0.4000\n",
            "Epoch [242/1000], Loss: 0.4376\n",
            "Epoch [243/1000], Loss: 0.4901\n",
            "Epoch [244/1000], Loss: 0.4366\n",
            "Epoch [245/1000], Loss: 0.4553\n",
            "Epoch [246/1000], Loss: 0.4530\n",
            "Epoch [247/1000], Loss: 0.4046\n",
            "Epoch [248/1000], Loss: 0.4417\n",
            "Epoch [249/1000], Loss: 0.4082\n",
            "Epoch [250/1000], Loss: 0.4477\n",
            "Epoch [251/1000], Loss: 0.4624\n",
            "Epoch [252/1000], Loss: 0.4096\n",
            "Epoch [253/1000], Loss: 0.4308\n",
            "Epoch [254/1000], Loss: 0.4452\n",
            "Epoch [255/1000], Loss: 0.4382\n",
            "Epoch [256/1000], Loss: 0.4187\n",
            "Epoch [257/1000], Loss: 0.4185\n",
            "Epoch [258/1000], Loss: 0.4288\n",
            "Epoch [259/1000], Loss: 0.4246\n",
            "Epoch [260/1000], Loss: 0.4508\n",
            "Epoch [261/1000], Loss: 0.4213\n",
            "Epoch [262/1000], Loss: 0.4364\n",
            "Epoch [263/1000], Loss: 0.4006\n",
            "Epoch [264/1000], Loss: 0.5210\n",
            "Epoch [265/1000], Loss: 0.4268\n",
            "Epoch [266/1000], Loss: 0.3991\n",
            "Epoch [267/1000], Loss: 0.4486\n",
            "Epoch [268/1000], Loss: 0.4447\n",
            "Epoch [269/1000], Loss: 0.3996\n",
            "Epoch [270/1000], Loss: 0.4109\n",
            "Epoch [271/1000], Loss: 0.4213\n",
            "Epoch [272/1000], Loss: 0.4665\n",
            "Epoch [273/1000], Loss: 0.4379\n",
            "Epoch [274/1000], Loss: 0.4159\n",
            "Epoch [275/1000], Loss: 0.4213\n",
            "Epoch [276/1000], Loss: 0.4174\n",
            "Epoch [277/1000], Loss: 0.4210\n",
            "Epoch [278/1000], Loss: 0.4159\n",
            "Epoch [279/1000], Loss: 0.4194\n",
            "Epoch [280/1000], Loss: 0.4118\n",
            "Epoch [281/1000], Loss: 0.4232\n",
            "Epoch [282/1000], Loss: 0.4232\n",
            "Epoch [283/1000], Loss: 0.3871\n",
            "Epoch [284/1000], Loss: 0.4456\n",
            "Epoch [285/1000], Loss: 0.4391\n",
            "Epoch [286/1000], Loss: 0.4145\n",
            "Epoch [287/1000], Loss: 0.4424\n",
            "Epoch [288/1000], Loss: 0.4161\n",
            "Epoch [289/1000], Loss: 0.4140\n",
            "Epoch [290/1000], Loss: 0.3970\n",
            "Epoch [291/1000], Loss: 0.4027\n",
            "Epoch [292/1000], Loss: 0.4564\n",
            "Epoch [293/1000], Loss: 0.4174\n",
            "Epoch [294/1000], Loss: 0.3996\n",
            "Epoch [295/1000], Loss: 0.4282\n",
            "Epoch [296/1000], Loss: 0.4127\n",
            "Epoch [297/1000], Loss: 0.4341\n",
            "Epoch [298/1000], Loss: 0.3985\n",
            "Epoch [299/1000], Loss: 0.4135\n",
            "Epoch [300/1000], Loss: 0.4308\n",
            "Epoch [301/1000], Loss: 0.4317\n",
            "Epoch [302/1000], Loss: 0.4045\n",
            "Epoch [303/1000], Loss: 0.4387\n",
            "Epoch [304/1000], Loss: 0.4250\n",
            "Epoch [305/1000], Loss: 0.4065\n",
            "Epoch [306/1000], Loss: 0.4225\n",
            "Epoch [307/1000], Loss: 0.3917\n",
            "Epoch [308/1000], Loss: 0.4065\n",
            "Epoch [309/1000], Loss: 0.4303\n",
            "Epoch [310/1000], Loss: 0.4058\n",
            "Epoch [311/1000], Loss: 0.4458\n",
            "Epoch [312/1000], Loss: 0.3946\n",
            "Epoch [313/1000], Loss: 0.3913\n",
            "Epoch [314/1000], Loss: 0.3971\n",
            "Epoch [315/1000], Loss: 0.4215\n",
            "Epoch [316/1000], Loss: 0.4024\n",
            "Epoch [317/1000], Loss: 0.3920\n",
            "Epoch [318/1000], Loss: 0.3996\n",
            "Epoch [319/1000], Loss: 0.4330\n",
            "Epoch [320/1000], Loss: 0.3868\n",
            "Epoch [321/1000], Loss: 0.4196\n",
            "Epoch [322/1000], Loss: 0.4119\n",
            "Epoch [323/1000], Loss: 0.4056\n",
            "Epoch [324/1000], Loss: 0.3970\n",
            "Epoch [325/1000], Loss: 0.3858\n",
            "Epoch [326/1000], Loss: 0.3984\n",
            "Epoch [327/1000], Loss: 0.3941\n",
            "Epoch [328/1000], Loss: 0.4177\n",
            "Epoch [329/1000], Loss: 0.4273\n",
            "Epoch [330/1000], Loss: 0.3917\n",
            "Epoch [331/1000], Loss: 0.4206\n",
            "Epoch [332/1000], Loss: 0.3875\n",
            "Epoch [333/1000], Loss: 0.3948\n",
            "Epoch [334/1000], Loss: 0.4000\n",
            "Epoch [335/1000], Loss: 0.4030\n",
            "Epoch [336/1000], Loss: 0.4071\n",
            "Epoch [337/1000], Loss: 0.3991\n",
            "Epoch [338/1000], Loss: 0.4035\n",
            "Epoch [339/1000], Loss: 0.3876\n",
            "Epoch [340/1000], Loss: 0.3914\n",
            "Epoch [341/1000], Loss: 0.4138\n",
            "Epoch [342/1000], Loss: 0.4238\n",
            "Epoch [343/1000], Loss: 0.4179\n",
            "Epoch [344/1000], Loss: 0.3949\n",
            "Epoch [345/1000], Loss: 0.3827\n",
            "Epoch [346/1000], Loss: 0.3866\n",
            "Epoch [347/1000], Loss: 0.3835\n",
            "Epoch [348/1000], Loss: 0.3932\n",
            "Epoch [349/1000], Loss: 0.4002\n",
            "Epoch [350/1000], Loss: 0.4059\n",
            "Epoch [351/1000], Loss: 0.4160\n",
            "Epoch [352/1000], Loss: 0.3816\n",
            "Epoch [353/1000], Loss: 0.4087\n",
            "Epoch [354/1000], Loss: 0.4029\n",
            "Epoch [355/1000], Loss: 0.3975\n",
            "Epoch [356/1000], Loss: 0.3923\n",
            "Epoch [357/1000], Loss: 0.3811\n",
            "Epoch [358/1000], Loss: 0.3935\n",
            "Epoch [359/1000], Loss: 0.3810\n",
            "Epoch [360/1000], Loss: 0.3957\n",
            "Epoch [361/1000], Loss: 0.3698\n",
            "Epoch [362/1000], Loss: 0.3896\n",
            "Epoch [363/1000], Loss: 0.4268\n",
            "Epoch [364/1000], Loss: 0.3924\n",
            "Epoch [365/1000], Loss: 0.3859\n",
            "Epoch [366/1000], Loss: 0.3942\n",
            "Epoch [367/1000], Loss: 0.4139\n",
            "Epoch [368/1000], Loss: 0.4037\n",
            "Epoch [369/1000], Loss: 0.3864\n",
            "Epoch [370/1000], Loss: 0.3830\n",
            "Epoch [371/1000], Loss: 0.4065\n",
            "Epoch [372/1000], Loss: 0.4309\n",
            "Epoch [373/1000], Loss: 0.4258\n",
            "Epoch [374/1000], Loss: 0.3751\n",
            "Epoch [375/1000], Loss: 0.4183\n",
            "Epoch [376/1000], Loss: 0.3829\n",
            "Epoch [377/1000], Loss: 0.3831\n",
            "Epoch [378/1000], Loss: 0.4056\n",
            "Epoch [379/1000], Loss: 0.3965\n",
            "Epoch [380/1000], Loss: 0.4018\n",
            "Epoch [381/1000], Loss: 0.3934\n",
            "Epoch [382/1000], Loss: 0.3765\n",
            "Epoch [383/1000], Loss: 0.4110\n",
            "Epoch [384/1000], Loss: 0.3736\n",
            "Epoch [385/1000], Loss: 0.3653\n",
            "Epoch [386/1000], Loss: 0.3967\n",
            "Epoch [387/1000], Loss: 0.3942\n",
            "Epoch [388/1000], Loss: 0.4045\n",
            "Epoch [389/1000], Loss: 0.3813\n",
            "Epoch [390/1000], Loss: 0.4150\n",
            "Epoch [391/1000], Loss: 0.3954\n",
            "Epoch [392/1000], Loss: 0.3678\n",
            "Epoch [393/1000], Loss: 0.3905\n",
            "Epoch [394/1000], Loss: 0.3903\n",
            "Epoch [395/1000], Loss: 0.3994\n",
            "Epoch [396/1000], Loss: 0.3830\n",
            "Epoch [397/1000], Loss: 0.3606\n",
            "Epoch [398/1000], Loss: 0.3831\n",
            "Epoch [399/1000], Loss: 0.3926\n",
            "Epoch [400/1000], Loss: 0.3821\n",
            "Epoch [401/1000], Loss: 0.4214\n",
            "Epoch [402/1000], Loss: 0.3584\n",
            "Epoch [403/1000], Loss: 0.3927\n",
            "Epoch [404/1000], Loss: 0.3720\n",
            "Epoch [405/1000], Loss: 0.3860\n",
            "Epoch [406/1000], Loss: 0.4248\n",
            "Epoch [407/1000], Loss: 0.3785\n",
            "Epoch [408/1000], Loss: 0.4077\n",
            "Epoch [409/1000], Loss: 0.3848\n",
            "Epoch [410/1000], Loss: 0.3754\n",
            "Epoch [411/1000], Loss: 0.3837\n",
            "Epoch [412/1000], Loss: 0.4149\n",
            "Epoch [413/1000], Loss: 0.3774\n",
            "Epoch [414/1000], Loss: 0.3588\n",
            "Epoch [415/1000], Loss: 0.4017\n",
            "Epoch [416/1000], Loss: 0.3881\n",
            "Epoch [417/1000], Loss: 0.4007\n",
            "Epoch [418/1000], Loss: 0.3806\n",
            "Epoch [419/1000], Loss: 0.3942\n",
            "Epoch [420/1000], Loss: 0.4144\n",
            "Epoch [421/1000], Loss: 0.3564\n",
            "Epoch [422/1000], Loss: 0.3832\n",
            "Epoch [423/1000], Loss: 0.3594\n",
            "Epoch [424/1000], Loss: 0.3939\n",
            "Epoch [425/1000], Loss: 0.3884\n",
            "Epoch [426/1000], Loss: 0.3903\n",
            "Epoch [427/1000], Loss: 0.3724\n",
            "Epoch [428/1000], Loss: 0.3887\n",
            "Epoch [429/1000], Loss: 0.4021\n",
            "Epoch [430/1000], Loss: 0.3666\n",
            "Epoch [431/1000], Loss: 0.4005\n",
            "Epoch [432/1000], Loss: 0.3970\n",
            "Epoch [433/1000], Loss: 0.3822\n",
            "Epoch [434/1000], Loss: 0.3592\n",
            "Epoch [435/1000], Loss: 0.3972\n",
            "Epoch [436/1000], Loss: 0.3672\n",
            "Epoch [437/1000], Loss: 0.3920\n",
            "Epoch [438/1000], Loss: 0.3967\n",
            "Epoch [439/1000], Loss: 0.3546\n",
            "Epoch [440/1000], Loss: 0.3667\n",
            "Epoch [441/1000], Loss: 0.3890\n",
            "Epoch [442/1000], Loss: 0.3838\n",
            "Epoch [443/1000], Loss: 0.3845\n",
            "Epoch [444/1000], Loss: 0.4037\n",
            "Epoch [445/1000], Loss: 0.3665\n",
            "Epoch [446/1000], Loss: 0.3741\n",
            "Epoch [447/1000], Loss: 0.3843\n",
            "Epoch [448/1000], Loss: 0.3614\n",
            "Epoch [449/1000], Loss: 0.3782\n",
            "Epoch [450/1000], Loss: 0.3598\n",
            "Epoch [451/1000], Loss: 0.3731\n",
            "Epoch [452/1000], Loss: 0.3887\n",
            "Epoch [453/1000], Loss: 0.3826\n",
            "Epoch [454/1000], Loss: 0.3919\n",
            "Epoch [455/1000], Loss: 0.3810\n",
            "Epoch [456/1000], Loss: 0.3813\n",
            "Epoch [457/1000], Loss: 0.3760\n",
            "Epoch [458/1000], Loss: 0.3689\n",
            "Epoch [459/1000], Loss: 0.3884\n",
            "Epoch [460/1000], Loss: 0.3901\n",
            "Epoch [461/1000], Loss: 0.3669\n",
            "Epoch [462/1000], Loss: 0.3946\n",
            "Epoch [463/1000], Loss: 0.3691\n",
            "Epoch [464/1000], Loss: 0.3777\n",
            "Epoch [465/1000], Loss: 0.3653\n",
            "Epoch [466/1000], Loss: 0.3682\n",
            "Epoch [467/1000], Loss: 0.3933\n",
            "Epoch [468/1000], Loss: 0.3757\n",
            "Epoch [469/1000], Loss: 0.3731\n",
            "Epoch [470/1000], Loss: 0.3680\n",
            "Epoch [471/1000], Loss: 0.3776\n",
            "Epoch [472/1000], Loss: 0.3787\n",
            "Epoch [473/1000], Loss: 0.3726\n",
            "Epoch [474/1000], Loss: 0.3838\n",
            "Epoch [475/1000], Loss: 0.3789\n",
            "Epoch [476/1000], Loss: 0.3847\n",
            "Epoch [477/1000], Loss: 0.3732\n",
            "Epoch [478/1000], Loss: 0.3703\n",
            "Epoch [479/1000], Loss: 0.3973\n",
            "Epoch [480/1000], Loss: 0.3662\n",
            "Epoch [481/1000], Loss: 0.4143\n",
            "Epoch [482/1000], Loss: 0.3639\n",
            "Epoch [483/1000], Loss: 0.3646\n",
            "Epoch [484/1000], Loss: 0.3749\n",
            "Epoch [485/1000], Loss: 0.3619\n",
            "Epoch [486/1000], Loss: 0.3767\n",
            "Epoch [487/1000], Loss: 0.3724\n",
            "Epoch [488/1000], Loss: 0.3757\n",
            "Epoch [489/1000], Loss: 0.3654\n",
            "Epoch [490/1000], Loss: 0.3805\n",
            "Epoch [491/1000], Loss: 0.3662\n",
            "Epoch [492/1000], Loss: 0.3846\n",
            "Epoch [493/1000], Loss: 0.3848\n",
            "Epoch [494/1000], Loss: 0.3775\n",
            "Epoch [495/1000], Loss: 0.3799\n",
            "Epoch [496/1000], Loss: 0.3719\n",
            "Epoch [497/1000], Loss: 0.3502\n",
            "Epoch [498/1000], Loss: 0.3645\n",
            "Epoch [499/1000], Loss: 0.3711\n",
            "Epoch [500/1000], Loss: 0.3836\n",
            "Epoch [501/1000], Loss: 0.3710\n",
            "Epoch [502/1000], Loss: 0.3758\n",
            "Epoch [503/1000], Loss: 0.3789\n",
            "Epoch [504/1000], Loss: 0.3908\n",
            "Epoch [505/1000], Loss: 0.3714\n",
            "Epoch [506/1000], Loss: 0.3624\n",
            "Epoch [507/1000], Loss: 0.3803\n",
            "Epoch [508/1000], Loss: 0.3764\n",
            "Epoch [509/1000], Loss: 0.3641\n",
            "Epoch [510/1000], Loss: 0.3644\n",
            "Epoch [511/1000], Loss: 0.3844\n",
            "Epoch [512/1000], Loss: 0.3546\n",
            "Epoch [513/1000], Loss: 0.3995\n",
            "Epoch [514/1000], Loss: 0.3571\n",
            "Epoch [515/1000], Loss: 0.3622\n",
            "Epoch [516/1000], Loss: 0.3881\n",
            "Epoch [517/1000], Loss: 0.3491\n",
            "Epoch [518/1000], Loss: 0.3880\n",
            "Epoch [519/1000], Loss: 0.3531\n",
            "Epoch [520/1000], Loss: 0.3701\n",
            "Epoch [521/1000], Loss: 0.3991\n",
            "Epoch [522/1000], Loss: 0.3497\n",
            "Epoch [523/1000], Loss: 0.3401\n",
            "Epoch [524/1000], Loss: 0.3860\n",
            "Epoch [525/1000], Loss: 0.3653\n",
            "Epoch [526/1000], Loss: 0.3714\n",
            "Epoch [527/1000], Loss: 0.3547\n",
            "Epoch [528/1000], Loss: 0.3624\n",
            "Epoch [529/1000], Loss: 0.3503\n",
            "Epoch [530/1000], Loss: 0.3578\n",
            "Epoch [531/1000], Loss: 0.3872\n",
            "Epoch [532/1000], Loss: 0.3704\n",
            "Epoch [533/1000], Loss: 0.3638\n",
            "Epoch [534/1000], Loss: 0.3626\n",
            "Epoch [535/1000], Loss: 0.3787\n",
            "Epoch [536/1000], Loss: 0.3712\n",
            "Epoch [537/1000], Loss: 0.3617\n",
            "Epoch [538/1000], Loss: 0.3852\n",
            "Epoch [539/1000], Loss: 0.3610\n",
            "Epoch [540/1000], Loss: 0.3622\n",
            "Epoch [541/1000], Loss: 0.3947\n",
            "Epoch [542/1000], Loss: 0.3915\n",
            "Epoch [543/1000], Loss: 0.3806\n",
            "Epoch [544/1000], Loss: 0.3912\n",
            "Epoch [545/1000], Loss: 0.3638\n",
            "Epoch [546/1000], Loss: 0.3739\n",
            "Epoch [547/1000], Loss: 0.3666\n",
            "Epoch [548/1000], Loss: 0.3679\n",
            "Epoch [549/1000], Loss: 0.3709\n",
            "Epoch [550/1000], Loss: 0.3801\n",
            "Epoch [551/1000], Loss: 0.3734\n",
            "Epoch [552/1000], Loss: 0.3659\n",
            "Epoch [553/1000], Loss: 0.3591\n",
            "Epoch [554/1000], Loss: 0.3672\n",
            "Epoch [555/1000], Loss: 0.3705\n",
            "Epoch [556/1000], Loss: 0.3664\n",
            "Epoch [557/1000], Loss: 0.3690\n",
            "Epoch [558/1000], Loss: 0.3641\n",
            "Epoch [559/1000], Loss: 0.3914\n",
            "Epoch [560/1000], Loss: 0.3713\n",
            "Epoch [561/1000], Loss: 0.3630\n",
            "Epoch [562/1000], Loss: 0.3874\n",
            "Epoch [563/1000], Loss: 0.3640\n",
            "Epoch [564/1000], Loss: 0.3501\n",
            "Epoch [565/1000], Loss: 0.3602\n",
            "Epoch [566/1000], Loss: 0.3729\n",
            "Epoch [567/1000], Loss: 0.3687\n",
            "Epoch [568/1000], Loss: 0.3599\n",
            "Epoch [569/1000], Loss: 0.3468\n",
            "Epoch [570/1000], Loss: 0.3865\n",
            "Epoch [571/1000], Loss: 0.3515\n",
            "Epoch [572/1000], Loss: 0.3598\n",
            "Epoch [573/1000], Loss: 0.3807\n",
            "Epoch [574/1000], Loss: 0.3703\n",
            "Epoch [575/1000], Loss: 0.3714\n",
            "Epoch [576/1000], Loss: 0.3715\n",
            "Epoch [577/1000], Loss: 0.3661\n",
            "Epoch [578/1000], Loss: 0.3696\n",
            "Epoch [579/1000], Loss: 0.3716\n",
            "Epoch [580/1000], Loss: 0.3628\n",
            "Epoch [581/1000], Loss: 0.3430\n",
            "Epoch [582/1000], Loss: 0.3661\n",
            "Epoch [583/1000], Loss: 0.4090\n",
            "Epoch [584/1000], Loss: 0.3587\n",
            "Epoch [585/1000], Loss: 0.3738\n",
            "Epoch [586/1000], Loss: 0.3790\n",
            "Epoch [587/1000], Loss: 0.3608\n",
            "Epoch [588/1000], Loss: 0.3751\n",
            "Epoch [589/1000], Loss: 0.3766\n",
            "Epoch [590/1000], Loss: 0.3558\n",
            "Epoch [591/1000], Loss: 0.3577\n",
            "Epoch [592/1000], Loss: 0.3834\n",
            "Epoch [593/1000], Loss: 0.3699\n",
            "Epoch [594/1000], Loss: 0.3532\n",
            "Epoch [595/1000], Loss: 0.3623\n",
            "Epoch [596/1000], Loss: 0.3843\n",
            "Epoch [597/1000], Loss: 0.3784\n",
            "Epoch [598/1000], Loss: 0.3704\n",
            "Epoch [599/1000], Loss: 0.3682\n",
            "Epoch [600/1000], Loss: 0.3727\n",
            "Epoch [601/1000], Loss: 0.3527\n",
            "Epoch [602/1000], Loss: 0.4038\n",
            "Epoch [603/1000], Loss: 0.3501\n",
            "Epoch [604/1000], Loss: 0.3562\n",
            "Epoch [605/1000], Loss: 0.3510\n",
            "Epoch [606/1000], Loss: 0.3763\n",
            "Epoch [607/1000], Loss: 0.3563\n",
            "Epoch [608/1000], Loss: 0.3609\n",
            "Epoch [609/1000], Loss: 0.4147\n",
            "Epoch [610/1000], Loss: 0.3519\n",
            "Epoch [611/1000], Loss: 0.3513\n",
            "Epoch [612/1000], Loss: 0.3663\n",
            "Epoch [613/1000], Loss: 0.3707\n",
            "Epoch [614/1000], Loss: 0.3455\n",
            "Epoch [615/1000], Loss: 0.3496\n",
            "Epoch [616/1000], Loss: 0.3763\n",
            "Epoch [617/1000], Loss: 0.3823\n",
            "Epoch [618/1000], Loss: 0.3607\n",
            "Epoch [619/1000], Loss: 0.3639\n",
            "Epoch [620/1000], Loss: 0.3672\n",
            "Epoch [621/1000], Loss: 0.3662\n",
            "Epoch [622/1000], Loss: 0.3785\n",
            "Epoch [623/1000], Loss: 0.3914\n",
            "Epoch [624/1000], Loss: 0.3737\n",
            "Epoch [625/1000], Loss: 0.3554\n",
            "Epoch [626/1000], Loss: 0.3474\n",
            "Epoch [627/1000], Loss: 0.3609\n",
            "Epoch [628/1000], Loss: 0.3551\n",
            "Epoch [629/1000], Loss: 0.3489\n",
            "Epoch [630/1000], Loss: 0.3828\n",
            "Epoch [631/1000], Loss: 0.3593\n",
            "Epoch [632/1000], Loss: 0.3540\n",
            "Epoch [633/1000], Loss: 0.3742\n",
            "Epoch [634/1000], Loss: 0.3513\n",
            "Epoch [635/1000], Loss: 0.3673\n",
            "Epoch [636/1000], Loss: 0.3679\n",
            "Epoch [637/1000], Loss: 0.3479\n",
            "Epoch [638/1000], Loss: 0.3602\n",
            "Epoch [639/1000], Loss: 0.3885\n",
            "Epoch [640/1000], Loss: 0.3725\n",
            "Epoch [641/1000], Loss: 0.3472\n",
            "Epoch [642/1000], Loss: 0.3501\n",
            "Epoch [643/1000], Loss: 0.3719\n",
            "Epoch [644/1000], Loss: 0.3860\n",
            "Epoch [645/1000], Loss: 0.3787\n",
            "Epoch [646/1000], Loss: 0.3866\n",
            "Epoch [647/1000], Loss: 0.3549\n",
            "Epoch [648/1000], Loss: 0.3586\n",
            "Epoch [649/1000], Loss: 0.3552\n",
            "Epoch [650/1000], Loss: 0.3638\n",
            "Epoch [651/1000], Loss: 0.3604\n",
            "Epoch [652/1000], Loss: 0.3667\n",
            "Epoch [653/1000], Loss: 0.3479\n",
            "Epoch [654/1000], Loss: 0.3419\n",
            "Epoch [655/1000], Loss: 0.3745\n",
            "Epoch [656/1000], Loss: 0.3646\n",
            "Epoch [657/1000], Loss: 0.3531\n",
            "Epoch [658/1000], Loss: 0.3643\n",
            "Epoch [659/1000], Loss: 0.3459\n",
            "Epoch [660/1000], Loss: 0.3510\n",
            "Epoch [661/1000], Loss: 0.3541\n",
            "Epoch [662/1000], Loss: 0.3865\n",
            "Epoch [663/1000], Loss: 0.3797\n",
            "Epoch [664/1000], Loss: 0.3627\n",
            "Epoch [665/1000], Loss: 0.3633\n",
            "Epoch [666/1000], Loss: 0.3615\n",
            "Epoch [667/1000], Loss: 0.3894\n",
            "Epoch [668/1000], Loss: 0.3500\n",
            "Epoch [669/1000], Loss: 0.3441\n",
            "Epoch [670/1000], Loss: 0.3537\n",
            "Epoch [671/1000], Loss: 0.3534\n",
            "Epoch [672/1000], Loss: 0.3655\n",
            "Epoch [673/1000], Loss: 0.3573\n",
            "Epoch [674/1000], Loss: 0.3593\n",
            "Epoch [675/1000], Loss: 0.3724\n",
            "Epoch [676/1000], Loss: 0.3704\n",
            "Epoch [677/1000], Loss: 0.3478\n",
            "Epoch [678/1000], Loss: 0.3686\n",
            "Epoch [679/1000], Loss: 0.3671\n",
            "Epoch [680/1000], Loss: 0.3556\n",
            "Epoch [681/1000], Loss: 0.3611\n",
            "Epoch [682/1000], Loss: 0.3760\n",
            "Epoch [683/1000], Loss: 0.3568\n",
            "Epoch [684/1000], Loss: 0.3444\n",
            "Epoch [685/1000], Loss: 0.3797\n",
            "Epoch [686/1000], Loss: 0.3549\n",
            "Epoch [687/1000], Loss: 0.3683\n",
            "Epoch [688/1000], Loss: 0.3537\n",
            "Epoch [689/1000], Loss: 0.3482\n",
            "Epoch [690/1000], Loss: 0.3511\n",
            "Epoch [691/1000], Loss: 0.3800\n",
            "Epoch [692/1000], Loss: 0.3624\n",
            "Epoch [693/1000], Loss: 0.3492\n",
            "Epoch [694/1000], Loss: 0.3586\n",
            "Epoch [695/1000], Loss: 0.3514\n",
            "Epoch [696/1000], Loss: 0.3634\n",
            "Epoch [697/1000], Loss: 0.3557\n",
            "Epoch [698/1000], Loss: 0.3557\n",
            "Epoch [699/1000], Loss: 0.3728\n",
            "Epoch [700/1000], Loss: 0.3593\n",
            "Epoch [701/1000], Loss: 0.3698\n",
            "Epoch [702/1000], Loss: 0.3961\n",
            "Epoch [703/1000], Loss: 0.3905\n",
            "Epoch [704/1000], Loss: 0.3661\n",
            "Epoch [705/1000], Loss: 0.3572\n",
            "Epoch [706/1000], Loss: 0.3657\n",
            "Epoch [707/1000], Loss: 0.3659\n",
            "Epoch [708/1000], Loss: 0.3640\n",
            "Epoch [709/1000], Loss: 0.3754\n",
            "Epoch [710/1000], Loss: 0.3465\n",
            "Epoch [711/1000], Loss: 0.3812\n",
            "Epoch [712/1000], Loss: 0.3703\n",
            "Epoch [713/1000], Loss: 0.3470\n",
            "Epoch [714/1000], Loss: 0.3760\n",
            "Epoch [715/1000], Loss: 0.3378\n",
            "Epoch [716/1000], Loss: 0.3566\n",
            "Epoch [717/1000], Loss: 0.3658\n",
            "Epoch [718/1000], Loss: 0.3652\n",
            "Epoch [719/1000], Loss: 0.3450\n",
            "Epoch [720/1000], Loss: 0.3674\n",
            "Epoch [721/1000], Loss: 0.3548\n",
            "Epoch [722/1000], Loss: 0.3442\n",
            "Epoch [723/1000], Loss: 0.3807\n",
            "Epoch [724/1000], Loss: 0.3463\n",
            "Epoch [725/1000], Loss: 0.3496\n",
            "Epoch [726/1000], Loss: 0.3630\n",
            "Epoch [727/1000], Loss: 0.3454\n",
            "Epoch [728/1000], Loss: 0.3482\n",
            "Epoch [729/1000], Loss: 0.3438\n",
            "Epoch [730/1000], Loss: 0.3722\n",
            "Epoch [731/1000], Loss: 0.3637\n",
            "Epoch [732/1000], Loss: 0.3655\n",
            "Epoch [733/1000], Loss: 0.3604\n",
            "Epoch [734/1000], Loss: 0.3462\n",
            "Epoch [735/1000], Loss: 0.3667\n",
            "Epoch [736/1000], Loss: 0.3498\n",
            "Epoch [737/1000], Loss: 0.3584\n",
            "Epoch [738/1000], Loss: 0.3539\n",
            "Epoch [739/1000], Loss: 0.3641\n",
            "Epoch [740/1000], Loss: 0.3556\n",
            "Epoch [741/1000], Loss: 0.3560\n",
            "Epoch [742/1000], Loss: 0.3559\n",
            "Epoch [743/1000], Loss: 0.3451\n",
            "Epoch [744/1000], Loss: 0.3589\n",
            "Epoch [745/1000], Loss: 0.3513\n",
            "Epoch [746/1000], Loss: 0.3611\n",
            "Epoch [747/1000], Loss: 0.3509\n",
            "Epoch [748/1000], Loss: 0.3524\n",
            "Epoch [749/1000], Loss: 0.3517\n",
            "Epoch [750/1000], Loss: 0.3668\n",
            "Epoch [751/1000], Loss: 0.3804\n",
            "Epoch [752/1000], Loss: 0.3502\n",
            "Epoch [753/1000], Loss: 0.3562\n",
            "Epoch [754/1000], Loss: 0.3455\n",
            "Epoch [755/1000], Loss: 0.3640\n",
            "Epoch [756/1000], Loss: 0.3560\n",
            "Epoch [757/1000], Loss: 0.3487\n",
            "Epoch [758/1000], Loss: 0.3561\n",
            "Epoch [759/1000], Loss: 0.3484\n",
            "Epoch [760/1000], Loss: 0.3828\n",
            "Epoch [761/1000], Loss: 0.3652\n",
            "Epoch [762/1000], Loss: 0.3685\n",
            "Epoch [763/1000], Loss: 0.3443\n",
            "Epoch [764/1000], Loss: 0.3531\n",
            "Epoch [765/1000], Loss: 0.3619\n",
            "Epoch [766/1000], Loss: 0.3649\n",
            "Epoch [767/1000], Loss: 0.3461\n",
            "Epoch [768/1000], Loss: 0.3561\n",
            "Epoch [769/1000], Loss: 0.3498\n",
            "Epoch [770/1000], Loss: 0.3557\n",
            "Epoch [771/1000], Loss: 0.3496\n",
            "Epoch [772/1000], Loss: 0.3581\n",
            "Epoch [773/1000], Loss: 0.3488\n",
            "Epoch [774/1000], Loss: 0.3544\n",
            "Epoch [775/1000], Loss: 0.3766\n",
            "Epoch [776/1000], Loss: 0.3615\n",
            "Epoch [777/1000], Loss: 0.3621\n",
            "Epoch [778/1000], Loss: 0.3391\n",
            "Epoch [779/1000], Loss: 0.3772\n",
            "Epoch [780/1000], Loss: 0.3587\n",
            "Epoch [781/1000], Loss: 0.3367\n",
            "Epoch [782/1000], Loss: 0.3733\n",
            "Epoch [783/1000], Loss: 0.3628\n",
            "Epoch [784/1000], Loss: 0.3720\n",
            "Epoch [785/1000], Loss: 0.3329\n",
            "Epoch [786/1000], Loss: 0.3421\n",
            "Epoch [787/1000], Loss: 0.3576\n",
            "Epoch [788/1000], Loss: 0.3660\n",
            "Epoch [789/1000], Loss: 0.3501\n",
            "Epoch [790/1000], Loss: 0.3604\n",
            "Epoch [791/1000], Loss: 0.3538\n",
            "Epoch [792/1000], Loss: 0.3658\n",
            "Epoch [793/1000], Loss: 0.3591\n",
            "Epoch [794/1000], Loss: 0.3530\n",
            "Epoch [795/1000], Loss: 0.3471\n",
            "Epoch [796/1000], Loss: 0.3515\n",
            "Epoch [797/1000], Loss: 0.3589\n",
            "Epoch [798/1000], Loss: 0.3389\n",
            "Epoch [799/1000], Loss: 0.3601\n",
            "Epoch [800/1000], Loss: 0.3555\n",
            "Epoch [801/1000], Loss: 0.3532\n",
            "Epoch [802/1000], Loss: 0.3921\n",
            "Epoch [803/1000], Loss: 0.3472\n",
            "Epoch [804/1000], Loss: 0.3401\n",
            "Epoch [805/1000], Loss: 0.3686\n",
            "Epoch [806/1000], Loss: 0.3455\n",
            "Epoch [807/1000], Loss: 0.3592\n",
            "Epoch [808/1000], Loss: 0.3512\n",
            "Epoch [809/1000], Loss: 0.3341\n",
            "Epoch [810/1000], Loss: 0.3551\n",
            "Epoch [811/1000], Loss: 0.3370\n",
            "Epoch [812/1000], Loss: 0.3792\n",
            "Epoch [813/1000], Loss: 0.3473\n",
            "Epoch [814/1000], Loss: 0.3518\n",
            "Epoch [815/1000], Loss: 0.3541\n",
            "Epoch [816/1000], Loss: 0.3777\n",
            "Epoch [817/1000], Loss: 0.3609\n",
            "Epoch [818/1000], Loss: 0.3436\n",
            "Epoch [819/1000], Loss: 0.3668\n",
            "Epoch [820/1000], Loss: 0.3487\n",
            "Epoch [821/1000], Loss: 0.3499\n",
            "Epoch [822/1000], Loss: 0.3366\n",
            "Epoch [823/1000], Loss: 0.3610\n",
            "Epoch [824/1000], Loss: 0.3693\n",
            "Epoch [825/1000], Loss: 0.3519\n",
            "Epoch [826/1000], Loss: 0.3528\n",
            "Epoch [827/1000], Loss: 0.3859\n",
            "Epoch [828/1000], Loss: 0.3471\n",
            "Epoch [829/1000], Loss: 0.3484\n",
            "Epoch [830/1000], Loss: 0.3698\n",
            "Epoch [831/1000], Loss: 0.3557\n",
            "Epoch [832/1000], Loss: 0.3479\n",
            "Epoch [833/1000], Loss: 0.3404\n",
            "Epoch [834/1000], Loss: 0.3722\n",
            "Epoch [835/1000], Loss: 0.3535\n",
            "Epoch [836/1000], Loss: 0.3538\n",
            "Epoch [837/1000], Loss: 0.3360\n",
            "Epoch [838/1000], Loss: 0.3585\n",
            "Epoch [839/1000], Loss: 0.3657\n",
            "Epoch [840/1000], Loss: 0.3494\n",
            "Epoch [841/1000], Loss: 0.3504\n",
            "Epoch [842/1000], Loss: 0.3554\n",
            "Epoch [843/1000], Loss: 0.3459\n",
            "Epoch [844/1000], Loss: 0.3589\n",
            "Epoch [845/1000], Loss: 0.3455\n",
            "Epoch [846/1000], Loss: 0.3515\n",
            "Epoch [847/1000], Loss: 0.3714\n",
            "Epoch [848/1000], Loss: 0.3371\n",
            "Epoch [849/1000], Loss: 0.3555\n",
            "Epoch [850/1000], Loss: 0.3400\n",
            "Epoch [851/1000], Loss: 0.3577\n",
            "Epoch [852/1000], Loss: 0.3684\n",
            "Epoch [853/1000], Loss: 0.3471\n",
            "Epoch [854/1000], Loss: 0.3372\n",
            "Epoch [855/1000], Loss: 0.3546\n",
            "Epoch [856/1000], Loss: 0.3716\n",
            "Epoch [857/1000], Loss: 0.3501\n",
            "Epoch [858/1000], Loss: 0.3871\n",
            "Epoch [859/1000], Loss: 0.3421\n",
            "Epoch [860/1000], Loss: 0.3463\n",
            "Epoch [861/1000], Loss: 0.3487\n",
            "Epoch [862/1000], Loss: 0.3367\n",
            "Epoch [863/1000], Loss: 0.3544\n",
            "Epoch [864/1000], Loss: 0.3553\n",
            "Epoch [865/1000], Loss: 0.3433\n",
            "Epoch [866/1000], Loss: 0.3451\n",
            "Epoch [867/1000], Loss: 0.3476\n",
            "Epoch [868/1000], Loss: 0.3467\n",
            "Epoch [869/1000], Loss: 0.3535\n",
            "Epoch [870/1000], Loss: 0.3358\n",
            "Epoch [871/1000], Loss: 0.3828\n",
            "Epoch [872/1000], Loss: 0.3482\n",
            "Epoch [873/1000], Loss: 0.3555\n",
            "Epoch [874/1000], Loss: 0.3443\n",
            "Epoch [875/1000], Loss: 0.3580\n",
            "Epoch [876/1000], Loss: 0.3355\n",
            "Epoch [877/1000], Loss: 0.3561\n",
            "Epoch [878/1000], Loss: 0.3516\n",
            "Epoch [879/1000], Loss: 0.3497\n",
            "Epoch [880/1000], Loss: 0.3535\n",
            "Epoch [881/1000], Loss: 0.3828\n",
            "Epoch [882/1000], Loss: 0.3380\n",
            "Epoch [883/1000], Loss: 0.3740\n",
            "Epoch [884/1000], Loss: 0.3312\n",
            "Epoch [885/1000], Loss: 0.3618\n",
            "Epoch [886/1000], Loss: 0.3567\n",
            "Epoch [887/1000], Loss: 0.3430\n",
            "Epoch [888/1000], Loss: 0.3402\n",
            "Epoch [889/1000], Loss: 0.3512\n",
            "Epoch [890/1000], Loss: 0.3672\n",
            "Epoch [891/1000], Loss: 0.3501\n",
            "Epoch [892/1000], Loss: 0.3652\n",
            "Epoch [893/1000], Loss: 0.3446\n",
            "Epoch [894/1000], Loss: 0.3519\n",
            "Epoch [895/1000], Loss: 0.3444\n",
            "Epoch [896/1000], Loss: 0.3580\n",
            "Epoch [897/1000], Loss: 0.3686\n",
            "Epoch [898/1000], Loss: 0.3487\n",
            "Epoch [899/1000], Loss: 0.3438\n",
            "Epoch [900/1000], Loss: 0.3746\n",
            "Epoch [901/1000], Loss: 0.3454\n",
            "Epoch [902/1000], Loss: 0.3504\n",
            "Epoch [903/1000], Loss: 0.3576\n",
            "Epoch [904/1000], Loss: 0.3588\n",
            "Epoch [905/1000], Loss: 0.3616\n",
            "Epoch [906/1000], Loss: 0.3554\n",
            "Epoch [907/1000], Loss: 0.3524\n",
            "Epoch [908/1000], Loss: 0.3266\n",
            "Epoch [909/1000], Loss: 0.3727\n",
            "Epoch [910/1000], Loss: 0.3800\n",
            "Epoch [911/1000], Loss: 0.3510\n",
            "Epoch [912/1000], Loss: 0.3489\n",
            "Epoch [913/1000], Loss: 0.3356\n",
            "Epoch [914/1000], Loss: 0.3585\n",
            "Epoch [915/1000], Loss: 0.3500\n",
            "Epoch [916/1000], Loss: 0.3429\n",
            "Epoch [917/1000], Loss: 0.3565\n",
            "Epoch [918/1000], Loss: 0.3439\n",
            "Epoch [919/1000], Loss: 0.3639\n",
            "Epoch [920/1000], Loss: 0.3366\n",
            "Epoch [921/1000], Loss: 0.3777\n",
            "Epoch [922/1000], Loss: 0.3730\n",
            "Epoch [923/1000], Loss: 0.3361\n",
            "Epoch [924/1000], Loss: 0.3513\n",
            "Epoch [925/1000], Loss: 0.3586\n",
            "Epoch [926/1000], Loss: 0.3307\n",
            "Epoch [927/1000], Loss: 0.3338\n",
            "Epoch [928/1000], Loss: 0.3303\n",
            "Epoch [929/1000], Loss: 0.3584\n",
            "Epoch [930/1000], Loss: 0.3440\n",
            "Epoch [931/1000], Loss: 0.3652\n",
            "Epoch [932/1000], Loss: 0.3427\n",
            "Epoch [933/1000], Loss: 0.3446\n",
            "Epoch [934/1000], Loss: 0.3431\n",
            "Epoch [935/1000], Loss: 0.3679\n",
            "Epoch [936/1000], Loss: 0.3352\n",
            "Epoch [937/1000], Loss: 0.3449\n",
            "Epoch [938/1000], Loss: 0.3626\n",
            "Epoch [939/1000], Loss: 0.3492\n",
            "Epoch [940/1000], Loss: 0.3353\n",
            "Epoch [941/1000], Loss: 0.3562\n",
            "Epoch [942/1000], Loss: 0.3742\n",
            "Epoch [943/1000], Loss: 0.3478\n",
            "Epoch [944/1000], Loss: 0.3495\n",
            "Epoch [945/1000], Loss: 0.3480\n",
            "Epoch [946/1000], Loss: 0.3632\n",
            "Epoch [947/1000], Loss: 0.3354\n",
            "Epoch [948/1000], Loss: 0.3320\n",
            "Epoch [949/1000], Loss: 0.3746\n",
            "Epoch [950/1000], Loss: 0.3548\n",
            "Epoch [951/1000], Loss: 0.3367\n",
            "Epoch [952/1000], Loss: 0.3678\n",
            "Epoch [953/1000], Loss: 0.3513\n",
            "Epoch [954/1000], Loss: 0.3429\n",
            "Epoch [955/1000], Loss: 0.3393\n",
            "Epoch [956/1000], Loss: 0.3309\n",
            "Epoch [957/1000], Loss: 0.3579\n",
            "Epoch [958/1000], Loss: 0.3613\n",
            "Epoch [959/1000], Loss: 0.3356\n",
            "Epoch [960/1000], Loss: 0.3694\n",
            "Epoch [961/1000], Loss: 0.3422\n",
            "Epoch [962/1000], Loss: 0.3801\n",
            "Epoch [963/1000], Loss: 0.3323\n",
            "Epoch [964/1000], Loss: 0.3506\n",
            "Epoch [965/1000], Loss: 0.3506\n",
            "Epoch [966/1000], Loss: 0.3674\n",
            "Epoch [967/1000], Loss: 0.3294\n",
            "Epoch [968/1000], Loss: 0.3499\n",
            "Epoch [969/1000], Loss: 0.3466\n",
            "Epoch [970/1000], Loss: 0.3614\n",
            "Epoch [971/1000], Loss: 0.3468\n",
            "Epoch [972/1000], Loss: 0.3504\n",
            "Epoch [973/1000], Loss: 0.3270\n",
            "Epoch [974/1000], Loss: 0.3385\n",
            "Epoch [975/1000], Loss: 0.3349\n",
            "Epoch [976/1000], Loss: 0.3366\n",
            "Epoch [977/1000], Loss: 0.3547\n",
            "Epoch [978/1000], Loss: 0.3637\n",
            "Epoch [979/1000], Loss: 0.3464\n",
            "Epoch [980/1000], Loss: 0.3505\n",
            "Epoch [981/1000], Loss: 0.3305\n",
            "Epoch [982/1000], Loss: 0.3423\n",
            "Epoch [983/1000], Loss: 0.3401\n",
            "Epoch [984/1000], Loss: 0.3404\n",
            "Epoch [985/1000], Loss: 0.3411\n",
            "Epoch [986/1000], Loss: 0.3893\n",
            "Epoch [987/1000], Loss: 0.3505\n",
            "Epoch [988/1000], Loss: 0.3592\n",
            "Epoch [989/1000], Loss: 0.3381\n",
            "Epoch [990/1000], Loss: 0.3513\n",
            "Epoch [991/1000], Loss: 0.3484\n",
            "Epoch [992/1000], Loss: 0.3450\n",
            "Epoch [993/1000], Loss: 0.3374\n",
            "Epoch [994/1000], Loss: 0.3539\n",
            "Epoch [995/1000], Loss: 0.3492\n",
            "Epoch [996/1000], Loss: 0.3763\n",
            "Epoch [997/1000], Loss: 0.3386\n",
            "Epoch [998/1000], Loss: 0.3450\n",
            "Epoch [999/1000], Loss: 0.3486\n",
            "Epoch [1000/1000], Loss: 0.3647\n",
            "Accuracy on test set: 0.8983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GUardar Modelo"
      ],
      "metadata": {
        "id": "fiV0YXOt0pGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'model.pth')\n"
      ],
      "metadata": {
        "id": "rgiv2gJ80lvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargar el modelo"
      ],
      "metadata": {
        "id": "bjZVvJG41NSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('model.pth')\n"
      ],
      "metadata": {
        "id": "t20A_KKM0oJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guarda los pesos del modelo"
      ],
      "metadata": {
        "id": "MyGqG4W31PEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model_weights.pth')\n"
      ],
      "metadata": {
        "id": "x8-xYRLc038S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exportar en formato ONNX"
      ],
      "metadata": {
        "id": "rMeNhqMp1dBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWLr-ltZ12pG",
        "outputId": "737eacde-a1b9-44f5-8c41-6c30680e5dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sample = torch.randn(1, input_size)  # Crear un tensor de muestra de entrada\n",
        "torch.onnx.export(model, input_sample, 'model.onnx')\n"
      ],
      "metadata": {
        "id": "YVy4pF3A07J0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import os\n",
        "\n",
        "# Cargar el conjunto de datos\n",
        "data = pd.read_csv(\"Dry_Bean_Dataset.csv\")\n",
        "\n",
        "# Preprocesamiento de datos\n",
        "label_encoder = LabelEncoder()\n",
        "data['Class'] = label_encoder.fit_transform(data['Class'])\n",
        "\n",
        "X = data.drop('Class', axis=1).values\n",
        "y = data['Class'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "class DatasetPersonalizado(torch.utils.data.Dataset):\n",
        "    # constructor\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = torch.from_numpy(X).float().cuda()\n",
        "        self.Y = torch.from_numpy(Y).long().cuda()\n",
        "    # devolvemos el número de datos en el dataset\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    # devolvemos el elemento `ix` del dataset\n",
        "    def __getitem__(self, ix):\n",
        "        return self.X[ix], self.Y[ix]\n",
        "\n",
        "train_dataset = DatasetPersonalizado(X_train, y_train)\n",
        "test_dataset = DatasetPersonalizado(X_test, y_test)\n",
        "\n",
        "# Definir el modelo MLP\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 50\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "model = MLP(input_size, hidden_size, num_classes).cuda()\n",
        "\n",
        "# Definir la función de pérdida y el optimizador\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "num_epochs = 1000\n",
        "checkpoint_interval = 20\n",
        "checkpoint_dir = 'checkpoints'\n",
        "\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in torch.utils.data.DataLoader(train_dataset, batch_size=62, shuffle=True):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
        "\n",
        "    # Guardar puntos de control\n",
        "    if (epoch + 1) % checkpoint_interval == 0:\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pt')\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': epoch_loss\n",
        "        }, checkpoint_path)\n",
        "\n",
        "# Evaluación del modelo\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in torch.utils.data.DataLoader(test_dataset, batch_size=62):\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Accuracy on test set: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3bpGyTZTgvJ",
        "outputId": "0ed02560-02fb-4111-98c3-18cb49e1d84b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1000], Loss: 650.2891\n",
            "Epoch [2/1000], Loss: 64.9436\n",
            "Epoch [3/1000], Loss: 60.4412\n",
            "Epoch [4/1000], Loss: 45.5871\n",
            "Epoch [5/1000], Loss: 46.0133\n",
            "Epoch [6/1000], Loss: 41.6313\n",
            "Epoch [7/1000], Loss: 42.9285\n",
            "Epoch [8/1000], Loss: 42.4867\n",
            "Epoch [9/1000], Loss: 32.8571\n",
            "Epoch [10/1000], Loss: 36.4204\n",
            "Epoch [11/1000], Loss: 32.1708\n",
            "Epoch [12/1000], Loss: 28.3282\n",
            "Epoch [13/1000], Loss: 20.3722\n",
            "Epoch [14/1000], Loss: 22.3866\n",
            "Epoch [15/1000], Loss: 21.9209\n",
            "Epoch [16/1000], Loss: 28.4712\n",
            "Epoch [17/1000], Loss: 22.7133\n",
            "Epoch [18/1000], Loss: 17.4238\n",
            "Epoch [19/1000], Loss: 25.1456\n",
            "Epoch [20/1000], Loss: 19.9404\n",
            "Epoch [21/1000], Loss: 14.5912\n",
            "Epoch [22/1000], Loss: 10.8420\n",
            "Epoch [23/1000], Loss: 22.6464\n",
            "Epoch [24/1000], Loss: 16.6444\n",
            "Epoch [25/1000], Loss: 12.6728\n",
            "Epoch [26/1000], Loss: 11.6991\n",
            "Epoch [27/1000], Loss: 22.5936\n",
            "Epoch [28/1000], Loss: 17.9194\n",
            "Epoch [29/1000], Loss: 18.5044\n",
            "Epoch [30/1000], Loss: 16.7435\n",
            "Epoch [31/1000], Loss: 16.5026\n",
            "Epoch [32/1000], Loss: 15.5127\n",
            "Epoch [33/1000], Loss: 11.6950\n",
            "Epoch [34/1000], Loss: 14.1221\n",
            "Epoch [35/1000], Loss: 13.6948\n",
            "Epoch [36/1000], Loss: 13.2323\n",
            "Epoch [37/1000], Loss: 10.6603\n",
            "Epoch [38/1000], Loss: 13.0534\n",
            "Epoch [39/1000], Loss: 14.2023\n",
            "Epoch [40/1000], Loss: 14.5776\n",
            "Epoch [41/1000], Loss: 20.4604\n",
            "Epoch [42/1000], Loss: 11.8455\n",
            "Epoch [43/1000], Loss: 11.6079\n",
            "Epoch [44/1000], Loss: 7.7808\n",
            "Epoch [45/1000], Loss: 12.4813\n",
            "Epoch [46/1000], Loss: 17.9732\n",
            "Epoch [47/1000], Loss: 10.5036\n",
            "Epoch [48/1000], Loss: 15.2442\n",
            "Epoch [49/1000], Loss: 9.9107\n",
            "Epoch [50/1000], Loss: 8.6610\n",
            "Epoch [51/1000], Loss: 12.8636\n",
            "Epoch [52/1000], Loss: 9.6876\n",
            "Epoch [53/1000], Loss: 10.4594\n",
            "Epoch [54/1000], Loss: 8.6080\n",
            "Epoch [55/1000], Loss: 10.1801\n",
            "Epoch [56/1000], Loss: 16.1010\n",
            "Epoch [57/1000], Loss: 9.9344\n",
            "Epoch [58/1000], Loss: 11.6559\n",
            "Epoch [59/1000], Loss: 12.1427\n",
            "Epoch [60/1000], Loss: 12.6014\n",
            "Epoch [61/1000], Loss: 8.8077\n",
            "Epoch [62/1000], Loss: 10.3461\n",
            "Epoch [63/1000], Loss: 9.6551\n",
            "Epoch [64/1000], Loss: 10.6637\n",
            "Epoch [65/1000], Loss: 8.3457\n",
            "Epoch [66/1000], Loss: 6.4656\n",
            "Epoch [67/1000], Loss: 8.8520\n",
            "Epoch [68/1000], Loss: 9.5949\n",
            "Epoch [69/1000], Loss: 7.8143\n",
            "Epoch [70/1000], Loss: 10.5505\n",
            "Epoch [71/1000], Loss: 8.2899\n",
            "Epoch [72/1000], Loss: 9.2428\n",
            "Epoch [73/1000], Loss: 7.1483\n",
            "Epoch [74/1000], Loss: 8.0432\n",
            "Epoch [75/1000], Loss: 8.3527\n",
            "Epoch [76/1000], Loss: 8.4616\n",
            "Epoch [77/1000], Loss: 7.9729\n",
            "Epoch [78/1000], Loss: 9.0667\n",
            "Epoch [79/1000], Loss: 9.7100\n",
            "Epoch [80/1000], Loss: 8.1873\n",
            "Epoch [81/1000], Loss: 9.7503\n",
            "Epoch [82/1000], Loss: 8.9361\n",
            "Epoch [83/1000], Loss: 6.4298\n",
            "Epoch [84/1000], Loss: 8.6933\n",
            "Epoch [85/1000], Loss: 6.6070\n",
            "Epoch [86/1000], Loss: 6.5444\n",
            "Epoch [87/1000], Loss: 5.9215\n",
            "Epoch [88/1000], Loss: 8.1650\n",
            "Epoch [89/1000], Loss: 7.5885\n",
            "Epoch [90/1000], Loss: 9.9439\n",
            "Epoch [91/1000], Loss: 5.9510\n",
            "Epoch [92/1000], Loss: 6.8762\n",
            "Epoch [93/1000], Loss: 6.8447\n",
            "Epoch [94/1000], Loss: 4.7642\n",
            "Epoch [95/1000], Loss: 5.8734\n",
            "Epoch [96/1000], Loss: 10.5114\n",
            "Epoch [97/1000], Loss: 6.5228\n",
            "Epoch [98/1000], Loss: 4.7076\n",
            "Epoch [99/1000], Loss: 7.3522\n",
            "Epoch [100/1000], Loss: 9.2879\n",
            "Epoch [101/1000], Loss: 5.5395\n",
            "Epoch [102/1000], Loss: 6.5579\n",
            "Epoch [103/1000], Loss: 4.1877\n",
            "Epoch [104/1000], Loss: 5.3692\n",
            "Epoch [105/1000], Loss: 5.5293\n",
            "Epoch [106/1000], Loss: 8.4104\n",
            "Epoch [107/1000], Loss: 5.8398\n",
            "Epoch [108/1000], Loss: 6.1325\n",
            "Epoch [109/1000], Loss: 4.2994\n",
            "Epoch [110/1000], Loss: 6.4676\n",
            "Epoch [111/1000], Loss: 6.0076\n",
            "Epoch [112/1000], Loss: 6.0511\n",
            "Epoch [113/1000], Loss: 6.8140\n",
            "Epoch [114/1000], Loss: 6.4983\n",
            "Epoch [115/1000], Loss: 7.2840\n",
            "Epoch [116/1000], Loss: 4.6014\n",
            "Epoch [117/1000], Loss: 5.4811\n",
            "Epoch [118/1000], Loss: 6.4870\n",
            "Epoch [119/1000], Loss: 5.0379\n",
            "Epoch [120/1000], Loss: 7.8615\n",
            "Epoch [121/1000], Loss: 5.3041\n",
            "Epoch [122/1000], Loss: 6.1854\n",
            "Epoch [123/1000], Loss: 4.7257\n",
            "Epoch [124/1000], Loss: 7.6054\n",
            "Epoch [125/1000], Loss: 4.8967\n",
            "Epoch [126/1000], Loss: 6.2925\n",
            "Epoch [127/1000], Loss: 4.2527\n",
            "Epoch [128/1000], Loss: 5.1667\n",
            "Epoch [129/1000], Loss: 6.1143\n",
            "Epoch [130/1000], Loss: 4.2201\n",
            "Epoch [131/1000], Loss: 5.5502\n",
            "Epoch [132/1000], Loss: 4.3933\n",
            "Epoch [133/1000], Loss: 4.9371\n",
            "Epoch [134/1000], Loss: 4.7836\n",
            "Epoch [135/1000], Loss: 3.7296\n",
            "Epoch [136/1000], Loss: 6.4230\n",
            "Epoch [137/1000], Loss: 6.7687\n",
            "Epoch [138/1000], Loss: 4.5343\n",
            "Epoch [139/1000], Loss: 3.9925\n",
            "Epoch [140/1000], Loss: 4.1902\n",
            "Epoch [141/1000], Loss: 4.1710\n",
            "Epoch [142/1000], Loss: 5.6810\n",
            "Epoch [143/1000], Loss: 4.7903\n",
            "Epoch [144/1000], Loss: 3.8884\n",
            "Epoch [145/1000], Loss: 5.5700\n",
            "Epoch [146/1000], Loss: 4.4962\n",
            "Epoch [147/1000], Loss: 5.6112\n",
            "Epoch [148/1000], Loss: 7.1693\n",
            "Epoch [149/1000], Loss: 4.6225\n",
            "Epoch [150/1000], Loss: 3.1599\n",
            "Epoch [151/1000], Loss: 3.6488\n",
            "Epoch [152/1000], Loss: 3.6678\n",
            "Epoch [153/1000], Loss: 4.0620\n",
            "Epoch [154/1000], Loss: 4.9589\n",
            "Epoch [155/1000], Loss: 5.9634\n",
            "Epoch [156/1000], Loss: 4.1550\n",
            "Epoch [157/1000], Loss: 5.4074\n",
            "Epoch [158/1000], Loss: 4.3797\n",
            "Epoch [159/1000], Loss: 3.2880\n",
            "Epoch [160/1000], Loss: 5.3939\n",
            "Epoch [161/1000], Loss: 3.3815\n",
            "Epoch [162/1000], Loss: 4.5290\n",
            "Epoch [163/1000], Loss: 4.0648\n",
            "Epoch [164/1000], Loss: 4.5683\n",
            "Epoch [165/1000], Loss: 3.0832\n",
            "Epoch [166/1000], Loss: 4.7626\n",
            "Epoch [167/1000], Loss: 5.2994\n",
            "Epoch [168/1000], Loss: 3.8952\n",
            "Epoch [169/1000], Loss: 2.8189\n",
            "Epoch [170/1000], Loss: 3.4848\n",
            "Epoch [171/1000], Loss: 4.2794\n",
            "Epoch [172/1000], Loss: 3.9517\n",
            "Epoch [173/1000], Loss: 4.5390\n",
            "Epoch [174/1000], Loss: 3.4640\n",
            "Epoch [175/1000], Loss: 3.6129\n",
            "Epoch [176/1000], Loss: 5.2961\n",
            "Epoch [177/1000], Loss: 3.6341\n",
            "Epoch [178/1000], Loss: 3.6523\n",
            "Epoch [179/1000], Loss: 4.7088\n",
            "Epoch [180/1000], Loss: 2.9719\n",
            "Epoch [181/1000], Loss: 4.7914\n",
            "Epoch [182/1000], Loss: 2.6733\n",
            "Epoch [183/1000], Loss: 4.7123\n",
            "Epoch [184/1000], Loss: 3.4473\n",
            "Epoch [185/1000], Loss: 3.8955\n",
            "Epoch [186/1000], Loss: 2.4482\n",
            "Epoch [187/1000], Loss: 2.9208\n",
            "Epoch [188/1000], Loss: 3.2024\n",
            "Epoch [189/1000], Loss: 4.0579\n",
            "Epoch [190/1000], Loss: 3.4967\n",
            "Epoch [191/1000], Loss: 2.5190\n",
            "Epoch [192/1000], Loss: 3.6635\n",
            "Epoch [193/1000], Loss: 3.0009\n",
            "Epoch [194/1000], Loss: 3.5730\n",
            "Epoch [195/1000], Loss: 3.0974\n",
            "Epoch [196/1000], Loss: 2.8607\n",
            "Epoch [197/1000], Loss: 3.1460\n",
            "Epoch [198/1000], Loss: 2.9227\n",
            "Epoch [199/1000], Loss: 2.5144\n",
            "Epoch [200/1000], Loss: 2.6361\n",
            "Epoch [201/1000], Loss: 2.6483\n",
            "Epoch [202/1000], Loss: 2.7674\n",
            "Epoch [203/1000], Loss: 2.8149\n",
            "Epoch [204/1000], Loss: 3.0117\n",
            "Epoch [205/1000], Loss: 2.3439\n",
            "Epoch [206/1000], Loss: 3.0052\n",
            "Epoch [207/1000], Loss: 3.3257\n",
            "Epoch [208/1000], Loss: 2.5603\n",
            "Epoch [209/1000], Loss: 2.5771\n",
            "Epoch [210/1000], Loss: 2.0215\n",
            "Epoch [211/1000], Loss: 2.3336\n",
            "Epoch [212/1000], Loss: 3.1509\n",
            "Epoch [213/1000], Loss: 2.5595\n",
            "Epoch [214/1000], Loss: 2.8601\n",
            "Epoch [215/1000], Loss: 2.0275\n",
            "Epoch [216/1000], Loss: 1.7067\n",
            "Epoch [217/1000], Loss: 3.0146\n",
            "Epoch [218/1000], Loss: 2.0603\n",
            "Epoch [219/1000], Loss: 2.0500\n",
            "Epoch [220/1000], Loss: 2.2177\n",
            "Epoch [221/1000], Loss: 2.6869\n",
            "Epoch [222/1000], Loss: 1.9245\n",
            "Epoch [223/1000], Loss: 2.1001\n",
            "Epoch [224/1000], Loss: 2.0633\n",
            "Epoch [225/1000], Loss: 1.8726\n",
            "Epoch [226/1000], Loss: 1.7623\n",
            "Epoch [227/1000], Loss: 2.2757\n",
            "Epoch [228/1000], Loss: 1.8835\n",
            "Epoch [229/1000], Loss: 2.0782\n",
            "Epoch [230/1000], Loss: 1.8230\n",
            "Epoch [231/1000], Loss: 1.7952\n",
            "Epoch [232/1000], Loss: 1.6308\n",
            "Epoch [233/1000], Loss: 1.6141\n",
            "Epoch [234/1000], Loss: 1.1870\n",
            "Epoch [235/1000], Loss: 1.4451\n",
            "Epoch [236/1000], Loss: 1.7308\n",
            "Epoch [237/1000], Loss: 1.5355\n",
            "Epoch [238/1000], Loss: 1.4827\n",
            "Epoch [239/1000], Loss: 1.4366\n",
            "Epoch [240/1000], Loss: 1.3529\n",
            "Epoch [241/1000], Loss: 1.5863\n",
            "Epoch [242/1000], Loss: 1.7740\n",
            "Epoch [243/1000], Loss: 2.1359\n",
            "Epoch [244/1000], Loss: 1.2764\n",
            "Epoch [245/1000], Loss: 1.1625\n",
            "Epoch [246/1000], Loss: 1.3326\n",
            "Epoch [247/1000], Loss: 1.2290\n",
            "Epoch [248/1000], Loss: 0.9715\n",
            "Epoch [249/1000], Loss: 1.0042\n",
            "Epoch [250/1000], Loss: 0.9274\n",
            "Epoch [251/1000], Loss: 1.1950\n",
            "Epoch [252/1000], Loss: 1.0709\n",
            "Epoch [253/1000], Loss: 0.9177\n",
            "Epoch [254/1000], Loss: 1.0550\n",
            "Epoch [255/1000], Loss: 1.0947\n",
            "Epoch [256/1000], Loss: 0.9547\n",
            "Epoch [257/1000], Loss: 0.8238\n",
            "Epoch [258/1000], Loss: 0.9710\n",
            "Epoch [259/1000], Loss: 0.6528\n",
            "Epoch [260/1000], Loss: 1.5113\n",
            "Epoch [261/1000], Loss: 0.8625\n",
            "Epoch [262/1000], Loss: 1.6751\n",
            "Epoch [263/1000], Loss: 0.7700\n",
            "Epoch [264/1000], Loss: 1.0164\n",
            "Epoch [265/1000], Loss: 0.7527\n",
            "Epoch [266/1000], Loss: 0.7354\n",
            "Epoch [267/1000], Loss: 0.7039\n",
            "Epoch [268/1000], Loss: 0.8929\n",
            "Epoch [269/1000], Loss: 0.7693\n",
            "Epoch [270/1000], Loss: 0.6963\n",
            "Epoch [271/1000], Loss: 0.7398\n",
            "Epoch [272/1000], Loss: 0.8175\n",
            "Epoch [273/1000], Loss: 0.5580\n",
            "Epoch [274/1000], Loss: 0.9398\n",
            "Epoch [275/1000], Loss: 0.7146\n",
            "Epoch [276/1000], Loss: 0.5706\n",
            "Epoch [277/1000], Loss: 0.4430\n",
            "Epoch [278/1000], Loss: 0.6985\n",
            "Epoch [279/1000], Loss: 0.5991\n",
            "Epoch [280/1000], Loss: 0.5023\n",
            "Epoch [281/1000], Loss: 0.9518\n",
            "Epoch [282/1000], Loss: 0.5535\n",
            "Epoch [283/1000], Loss: 0.4991\n",
            "Epoch [284/1000], Loss: 0.4083\n",
            "Epoch [285/1000], Loss: 0.4182\n",
            "Epoch [286/1000], Loss: 0.3825\n",
            "Epoch [287/1000], Loss: 0.4414\n",
            "Epoch [288/1000], Loss: 0.3646\n",
            "Epoch [289/1000], Loss: 0.4202\n",
            "Epoch [290/1000], Loss: 0.3594\n",
            "Epoch [291/1000], Loss: 0.3753\n",
            "Epoch [292/1000], Loss: 0.3807\n",
            "Epoch [293/1000], Loss: 0.4187\n",
            "Epoch [294/1000], Loss: 0.3607\n",
            "Epoch [295/1000], Loss: 0.4010\n",
            "Epoch [296/1000], Loss: 0.3804\n",
            "Epoch [297/1000], Loss: 0.3899\n",
            "Epoch [298/1000], Loss: 0.3772\n",
            "Epoch [299/1000], Loss: 0.3529\n",
            "Epoch [300/1000], Loss: 0.3921\n",
            "Epoch [301/1000], Loss: 0.3788\n",
            "Epoch [302/1000], Loss: 0.3918\n",
            "Epoch [303/1000], Loss: 0.3786\n",
            "Epoch [304/1000], Loss: 0.3840\n",
            "Epoch [305/1000], Loss: 0.3758\n",
            "Epoch [306/1000], Loss: 0.3737\n",
            "Epoch [307/1000], Loss: 0.3990\n",
            "Epoch [308/1000], Loss: 0.3848\n",
            "Epoch [309/1000], Loss: 0.4011\n",
            "Epoch [310/1000], Loss: 0.3525\n",
            "Epoch [311/1000], Loss: 0.3873\n",
            "Epoch [312/1000], Loss: 0.3683\n",
            "Epoch [313/1000], Loss: 0.3803\n",
            "Epoch [314/1000], Loss: 0.3928\n",
            "Epoch [315/1000], Loss: 0.3960\n",
            "Epoch [316/1000], Loss: 0.3686\n",
            "Epoch [317/1000], Loss: 0.3984\n",
            "Epoch [318/1000], Loss: 0.3986\n",
            "Epoch [319/1000], Loss: 0.4139\n",
            "Epoch [320/1000], Loss: 0.3848\n",
            "Epoch [321/1000], Loss: 0.3829\n",
            "Epoch [322/1000], Loss: 0.3784\n",
            "Epoch [323/1000], Loss: 0.3933\n",
            "Epoch [324/1000], Loss: 0.4165\n",
            "Epoch [325/1000], Loss: 0.3725\n",
            "Epoch [326/1000], Loss: 0.4344\n",
            "Epoch [327/1000], Loss: 0.3970\n",
            "Epoch [328/1000], Loss: 0.4201\n",
            "Epoch [329/1000], Loss: 0.3838\n",
            "Epoch [330/1000], Loss: 0.4194\n",
            "Epoch [331/1000], Loss: 0.3887\n",
            "Epoch [332/1000], Loss: 0.3866\n",
            "Epoch [333/1000], Loss: 0.3713\n",
            "Epoch [334/1000], Loss: 0.3692\n",
            "Epoch [335/1000], Loss: 0.4045\n",
            "Epoch [336/1000], Loss: 0.3772\n",
            "Epoch [337/1000], Loss: 0.3749\n",
            "Epoch [338/1000], Loss: 0.4044\n",
            "Epoch [339/1000], Loss: 0.3576\n",
            "Epoch [340/1000], Loss: 0.3758\n",
            "Epoch [341/1000], Loss: 0.3781\n",
            "Epoch [342/1000], Loss: 0.3748\n",
            "Epoch [343/1000], Loss: 0.4209\n",
            "Epoch [344/1000], Loss: 0.3643\n",
            "Epoch [345/1000], Loss: 0.4136\n",
            "Epoch [346/1000], Loss: 0.4013\n",
            "Epoch [347/1000], Loss: 0.3860\n",
            "Epoch [348/1000], Loss: 0.3987\n",
            "Epoch [349/1000], Loss: 0.3813\n",
            "Epoch [350/1000], Loss: 0.3708\n",
            "Epoch [351/1000], Loss: 0.3849\n",
            "Epoch [352/1000], Loss: 0.3990\n",
            "Epoch [353/1000], Loss: 0.3850\n",
            "Epoch [354/1000], Loss: 0.4527\n",
            "Epoch [355/1000], Loss: 0.3679\n",
            "Epoch [356/1000], Loss: 0.3559\n",
            "Epoch [357/1000], Loss: 0.4173\n",
            "Epoch [358/1000], Loss: 0.3619\n",
            "Epoch [359/1000], Loss: 0.3661\n",
            "Epoch [360/1000], Loss: 0.4118\n",
            "Epoch [361/1000], Loss: 0.3849\n",
            "Epoch [362/1000], Loss: 0.3716\n",
            "Epoch [363/1000], Loss: 0.4023\n",
            "Epoch [364/1000], Loss: 0.3780\n",
            "Epoch [365/1000], Loss: 0.3745\n",
            "Epoch [366/1000], Loss: 0.3920\n",
            "Epoch [367/1000], Loss: 0.3857\n",
            "Epoch [368/1000], Loss: 0.3646\n",
            "Epoch [369/1000], Loss: 0.3696\n",
            "Epoch [370/1000], Loss: 0.3559\n",
            "Epoch [371/1000], Loss: 0.3686\n",
            "Epoch [372/1000], Loss: 0.4246\n",
            "Epoch [373/1000], Loss: 0.3639\n",
            "Epoch [374/1000], Loss: 0.3970\n",
            "Epoch [375/1000], Loss: 0.4022\n",
            "Epoch [376/1000], Loss: 0.3917\n",
            "Epoch [377/1000], Loss: 0.3865\n",
            "Epoch [378/1000], Loss: 0.3775\n",
            "Epoch [379/1000], Loss: 0.3772\n",
            "Epoch [380/1000], Loss: 0.3780\n",
            "Epoch [381/1000], Loss: 0.4575\n",
            "Epoch [382/1000], Loss: 0.3973\n",
            "Epoch [383/1000], Loss: 0.3711\n",
            "Epoch [384/1000], Loss: 0.3794\n",
            "Epoch [385/1000], Loss: 0.3633\n",
            "Epoch [386/1000], Loss: 0.3640\n",
            "Epoch [387/1000], Loss: 0.3622\n",
            "Epoch [388/1000], Loss: 0.3806\n",
            "Epoch [389/1000], Loss: 0.3803\n",
            "Epoch [390/1000], Loss: 0.3623\n",
            "Epoch [391/1000], Loss: 0.3694\n",
            "Epoch [392/1000], Loss: 0.3842\n",
            "Epoch [393/1000], Loss: 0.3864\n",
            "Epoch [394/1000], Loss: 0.3746\n",
            "Epoch [395/1000], Loss: 0.3928\n",
            "Epoch [396/1000], Loss: 0.3656\n",
            "Epoch [397/1000], Loss: 0.3488\n",
            "Epoch [398/1000], Loss: 0.3550\n",
            "Epoch [399/1000], Loss: 0.3994\n",
            "Epoch [400/1000], Loss: 0.3765\n",
            "Epoch [401/1000], Loss: 0.3688\n",
            "Epoch [402/1000], Loss: 0.3565\n",
            "Epoch [403/1000], Loss: 0.4053\n",
            "Epoch [404/1000], Loss: 0.4065\n",
            "Epoch [405/1000], Loss: 0.3883\n",
            "Epoch [406/1000], Loss: 0.3671\n",
            "Epoch [407/1000], Loss: 0.3776\n",
            "Epoch [408/1000], Loss: 0.3552\n",
            "Epoch [409/1000], Loss: 0.3795\n",
            "Epoch [410/1000], Loss: 0.3649\n",
            "Epoch [411/1000], Loss: 0.3839\n",
            "Epoch [412/1000], Loss: 0.3599\n",
            "Epoch [413/1000], Loss: 0.3570\n",
            "Epoch [414/1000], Loss: 0.3760\n",
            "Epoch [415/1000], Loss: 0.3621\n",
            "Epoch [416/1000], Loss: 0.3655\n",
            "Epoch [417/1000], Loss: 0.3858\n",
            "Epoch [418/1000], Loss: 0.3728\n",
            "Epoch [419/1000], Loss: 0.3861\n",
            "Epoch [420/1000], Loss: 0.3726\n",
            "Epoch [421/1000], Loss: 0.3602\n",
            "Epoch [422/1000], Loss: 0.3597\n",
            "Epoch [423/1000], Loss: 0.3569\n",
            "Epoch [424/1000], Loss: 0.4049\n",
            "Epoch [425/1000], Loss: 0.3393\n",
            "Epoch [426/1000], Loss: 0.3609\n",
            "Epoch [427/1000], Loss: 0.3574\n",
            "Epoch [428/1000], Loss: 0.3548\n",
            "Epoch [429/1000], Loss: 0.4228\n",
            "Epoch [430/1000], Loss: 0.3667\n",
            "Epoch [431/1000], Loss: 0.3749\n",
            "Epoch [432/1000], Loss: 0.3738\n",
            "Epoch [433/1000], Loss: 0.3741\n",
            "Epoch [434/1000], Loss: 0.3787\n",
            "Epoch [435/1000], Loss: 0.3482\n",
            "Epoch [436/1000], Loss: 0.3865\n",
            "Epoch [437/1000], Loss: 0.3420\n",
            "Epoch [438/1000], Loss: 0.3709\n",
            "Epoch [439/1000], Loss: 0.4002\n",
            "Epoch [440/1000], Loss: 0.3737\n",
            "Epoch [441/1000], Loss: 0.3565\n",
            "Epoch [442/1000], Loss: 0.3796\n",
            "Epoch [443/1000], Loss: 0.3630\n",
            "Epoch [444/1000], Loss: 0.3976\n",
            "Epoch [445/1000], Loss: 0.3495\n",
            "Epoch [446/1000], Loss: 0.3501\n",
            "Epoch [447/1000], Loss: 0.3851\n",
            "Epoch [448/1000], Loss: 0.4058\n",
            "Epoch [449/1000], Loss: 0.3447\n",
            "Epoch [450/1000], Loss: 0.3671\n",
            "Epoch [451/1000], Loss: 0.3478\n",
            "Epoch [452/1000], Loss: 0.4049\n",
            "Epoch [453/1000], Loss: 0.3704\n",
            "Epoch [454/1000], Loss: 0.3472\n",
            "Epoch [455/1000], Loss: 0.3798\n",
            "Epoch [456/1000], Loss: 0.3530\n",
            "Epoch [457/1000], Loss: 0.3467\n",
            "Epoch [458/1000], Loss: 0.4106\n",
            "Epoch [459/1000], Loss: 0.3648\n",
            "Epoch [460/1000], Loss: 0.3779\n",
            "Epoch [461/1000], Loss: 0.3802\n",
            "Epoch [462/1000], Loss: 0.3572\n",
            "Epoch [463/1000], Loss: 0.3413\n",
            "Epoch [464/1000], Loss: 0.3835\n",
            "Epoch [465/1000], Loss: 0.3590\n",
            "Epoch [466/1000], Loss: 0.3516\n",
            "Epoch [467/1000], Loss: 0.3750\n",
            "Epoch [468/1000], Loss: 0.3616\n",
            "Epoch [469/1000], Loss: 0.3676\n",
            "Epoch [470/1000], Loss: 0.3510\n",
            "Epoch [471/1000], Loss: 0.3592\n",
            "Epoch [472/1000], Loss: 0.3487\n",
            "Epoch [473/1000], Loss: 0.3887\n",
            "Epoch [474/1000], Loss: 0.3336\n",
            "Epoch [475/1000], Loss: 0.3757\n",
            "Epoch [476/1000], Loss: 0.3725\n",
            "Epoch [477/1000], Loss: 0.3476\n",
            "Epoch [478/1000], Loss: 0.3876\n",
            "Epoch [479/1000], Loss: 0.3709\n",
            "Epoch [480/1000], Loss: 0.3784\n",
            "Epoch [481/1000], Loss: 0.3754\n",
            "Epoch [482/1000], Loss: 0.3689\n",
            "Epoch [483/1000], Loss: 0.3622\n",
            "Epoch [484/1000], Loss: 0.3570\n",
            "Epoch [485/1000], Loss: 0.3411\n",
            "Epoch [486/1000], Loss: 0.3461\n",
            "Epoch [487/1000], Loss: 0.3616\n",
            "Epoch [488/1000], Loss: 0.3751\n",
            "Epoch [489/1000], Loss: 0.3843\n",
            "Epoch [490/1000], Loss: 0.3403\n",
            "Epoch [491/1000], Loss: 0.3705\n",
            "Epoch [492/1000], Loss: 0.3894\n",
            "Epoch [493/1000], Loss: 0.3689\n",
            "Epoch [494/1000], Loss: 0.3603\n",
            "Epoch [495/1000], Loss: 0.3688\n",
            "Epoch [496/1000], Loss: 0.3819\n",
            "Epoch [497/1000], Loss: 0.3881\n",
            "Epoch [498/1000], Loss: 0.3753\n",
            "Epoch [499/1000], Loss: 0.3626\n",
            "Epoch [500/1000], Loss: 0.3897\n",
            "Epoch [501/1000], Loss: 0.3748\n",
            "Epoch [502/1000], Loss: 0.3714\n",
            "Epoch [503/1000], Loss: 0.3639\n",
            "Epoch [504/1000], Loss: 0.3723\n",
            "Epoch [505/1000], Loss: 0.3744\n",
            "Epoch [506/1000], Loss: 0.3585\n",
            "Epoch [507/1000], Loss: 0.3538\n",
            "Epoch [508/1000], Loss: 0.3728\n",
            "Epoch [509/1000], Loss: 0.3566\n",
            "Epoch [510/1000], Loss: 0.3500\n",
            "Epoch [511/1000], Loss: 0.3596\n",
            "Epoch [512/1000], Loss: 0.3557\n",
            "Epoch [513/1000], Loss: 0.3546\n",
            "Epoch [514/1000], Loss: 0.3413\n",
            "Epoch [515/1000], Loss: 0.3696\n",
            "Epoch [516/1000], Loss: 0.3796\n",
            "Epoch [517/1000], Loss: 0.3535\n",
            "Epoch [518/1000], Loss: 0.3688\n",
            "Epoch [519/1000], Loss: 0.3487\n",
            "Epoch [520/1000], Loss: 0.3895\n",
            "Epoch [521/1000], Loss: 0.3701\n",
            "Epoch [522/1000], Loss: 0.3558\n",
            "Epoch [523/1000], Loss: 0.3702\n",
            "Epoch [524/1000], Loss: 0.3226\n",
            "Epoch [525/1000], Loss: 0.3632\n",
            "Epoch [526/1000], Loss: 0.3662\n",
            "Epoch [527/1000], Loss: 0.3750\n",
            "Epoch [528/1000], Loss: 0.3602\n",
            "Epoch [529/1000], Loss: 0.3945\n",
            "Epoch [530/1000], Loss: 0.3724\n",
            "Epoch [531/1000], Loss: 0.3536\n",
            "Epoch [532/1000], Loss: 0.3360\n",
            "Epoch [533/1000], Loss: 0.3455\n",
            "Epoch [534/1000], Loss: 0.3705\n",
            "Epoch [535/1000], Loss: 0.3372\n",
            "Epoch [536/1000], Loss: 0.3594\n",
            "Epoch [537/1000], Loss: 0.3690\n",
            "Epoch [538/1000], Loss: 0.3700\n",
            "Epoch [539/1000], Loss: 0.3722\n",
            "Epoch [540/1000], Loss: 0.3523\n",
            "Epoch [541/1000], Loss: 0.3441\n",
            "Epoch [542/1000], Loss: 0.3660\n",
            "Epoch [543/1000], Loss: 0.3360\n",
            "Epoch [544/1000], Loss: 0.3650\n",
            "Epoch [545/1000], Loss: 0.3838\n",
            "Epoch [546/1000], Loss: 0.3413\n",
            "Epoch [547/1000], Loss: 0.3371\n",
            "Epoch [548/1000], Loss: 0.3407\n",
            "Epoch [549/1000], Loss: 0.3583\n",
            "Epoch [550/1000], Loss: 0.3725\n",
            "Epoch [551/1000], Loss: 0.3954\n",
            "Epoch [552/1000], Loss: 0.3549\n",
            "Epoch [553/1000], Loss: 0.3627\n",
            "Epoch [554/1000], Loss: 0.3605\n",
            "Epoch [555/1000], Loss: 0.3530\n",
            "Epoch [556/1000], Loss: 0.3707\n",
            "Epoch [557/1000], Loss: 0.3569\n",
            "Epoch [558/1000], Loss: 0.3566\n",
            "Epoch [559/1000], Loss: 0.3594\n",
            "Epoch [560/1000], Loss: 0.3526\n",
            "Epoch [561/1000], Loss: 0.3852\n",
            "Epoch [562/1000], Loss: 0.3737\n",
            "Epoch [563/1000], Loss: 0.3324\n",
            "Epoch [564/1000], Loss: 0.3506\n",
            "Epoch [565/1000], Loss: 0.3872\n",
            "Epoch [566/1000], Loss: 0.3412\n",
            "Epoch [567/1000], Loss: 0.3612\n",
            "Epoch [568/1000], Loss: 0.3577\n",
            "Epoch [569/1000], Loss: 0.3623\n",
            "Epoch [570/1000], Loss: 0.3503\n",
            "Epoch [571/1000], Loss: 0.3448\n",
            "Epoch [572/1000], Loss: 0.3665\n",
            "Epoch [573/1000], Loss: 0.3503\n",
            "Epoch [574/1000], Loss: 0.3563\n",
            "Epoch [575/1000], Loss: 0.3393\n",
            "Epoch [576/1000], Loss: 0.4057\n",
            "Epoch [577/1000], Loss: 0.3550\n",
            "Epoch [578/1000], Loss: 0.3839\n",
            "Epoch [579/1000], Loss: 0.3667\n",
            "Epoch [580/1000], Loss: 0.3736\n",
            "Epoch [581/1000], Loss: 0.3915\n",
            "Epoch [582/1000], Loss: 0.3746\n",
            "Epoch [583/1000], Loss: 0.3528\n",
            "Epoch [584/1000], Loss: 0.3422\n",
            "Epoch [585/1000], Loss: 0.3436\n",
            "Epoch [586/1000], Loss: 0.3573\n",
            "Epoch [587/1000], Loss: 0.3510\n",
            "Epoch [588/1000], Loss: 0.3562\n",
            "Epoch [589/1000], Loss: 0.3444\n",
            "Epoch [590/1000], Loss: 0.3919\n",
            "Epoch [591/1000], Loss: 0.3484\n",
            "Epoch [592/1000], Loss: 0.3190\n",
            "Epoch [593/1000], Loss: 0.3630\n",
            "Epoch [594/1000], Loss: 0.3720\n",
            "Epoch [595/1000], Loss: 0.3758\n",
            "Epoch [596/1000], Loss: 0.3842\n",
            "Epoch [597/1000], Loss: 0.3740\n",
            "Epoch [598/1000], Loss: 0.3444\n",
            "Epoch [599/1000], Loss: 0.3421\n",
            "Epoch [600/1000], Loss: 0.4092\n",
            "Epoch [601/1000], Loss: 0.3247\n",
            "Epoch [602/1000], Loss: 0.3683\n",
            "Epoch [603/1000], Loss: 0.3618\n",
            "Epoch [604/1000], Loss: 0.3429\n",
            "Epoch [605/1000], Loss: 0.3446\n",
            "Epoch [606/1000], Loss: 0.3255\n",
            "Epoch [607/1000], Loss: 0.3510\n",
            "Epoch [608/1000], Loss: 0.3463\n",
            "Epoch [609/1000], Loss: 0.3393\n",
            "Epoch [610/1000], Loss: 0.3646\n",
            "Epoch [611/1000], Loss: 0.3503\n",
            "Epoch [612/1000], Loss: 0.3673\n",
            "Epoch [613/1000], Loss: 0.3502\n",
            "Epoch [614/1000], Loss: 0.3444\n",
            "Epoch [615/1000], Loss: 0.3657\n",
            "Epoch [616/1000], Loss: 0.3660\n",
            "Epoch [617/1000], Loss: 0.3310\n",
            "Epoch [618/1000], Loss: 0.3382\n",
            "Epoch [619/1000], Loss: 0.3631\n",
            "Epoch [620/1000], Loss: 0.3656\n",
            "Epoch [621/1000], Loss: 0.3350\n",
            "Epoch [622/1000], Loss: 0.3479\n",
            "Epoch [623/1000], Loss: 0.3366\n",
            "Epoch [624/1000], Loss: 0.3401\n",
            "Epoch [625/1000], Loss: 0.3369\n",
            "Epoch [626/1000], Loss: 0.3570\n",
            "Epoch [627/1000], Loss: 0.4030\n",
            "Epoch [628/1000], Loss: 0.3295\n",
            "Epoch [629/1000], Loss: 0.3481\n",
            "Epoch [630/1000], Loss: 0.3327\n",
            "Epoch [631/1000], Loss: 0.3412\n",
            "Epoch [632/1000], Loss: 0.3420\n",
            "Epoch [633/1000], Loss: 0.3331\n",
            "Epoch [634/1000], Loss: 0.3612\n",
            "Epoch [635/1000], Loss: 0.3454\n",
            "Epoch [636/1000], Loss: 0.3496\n",
            "Epoch [637/1000], Loss: 0.3949\n",
            "Epoch [638/1000], Loss: 0.3525\n",
            "Epoch [639/1000], Loss: 0.3481\n",
            "Epoch [640/1000], Loss: 0.3524\n",
            "Epoch [641/1000], Loss: 0.3449\n",
            "Epoch [642/1000], Loss: 0.3558\n",
            "Epoch [643/1000], Loss: 0.3467\n",
            "Epoch [644/1000], Loss: 0.3436\n",
            "Epoch [645/1000], Loss: 0.3388\n",
            "Epoch [646/1000], Loss: 0.3837\n",
            "Epoch [647/1000], Loss: 0.3406\n",
            "Epoch [648/1000], Loss: 0.3511\n",
            "Epoch [649/1000], Loss: 0.3418\n",
            "Epoch [650/1000], Loss: 0.3570\n",
            "Epoch [651/1000], Loss: 0.3493\n",
            "Epoch [652/1000], Loss: 0.3540\n",
            "Epoch [653/1000], Loss: 0.3560\n",
            "Epoch [654/1000], Loss: 0.3388\n",
            "Epoch [655/1000], Loss: 0.3521\n",
            "Epoch [656/1000], Loss: 0.3605\n",
            "Epoch [657/1000], Loss: 0.3390\n",
            "Epoch [658/1000], Loss: 0.3779\n",
            "Epoch [659/1000], Loss: 0.3369\n",
            "Epoch [660/1000], Loss: 0.3421\n",
            "Epoch [661/1000], Loss: 0.3641\n",
            "Epoch [662/1000], Loss: 0.3663\n",
            "Epoch [663/1000], Loss: 0.3632\n",
            "Epoch [664/1000], Loss: 0.3447\n",
            "Epoch [665/1000], Loss: 0.3326\n",
            "Epoch [666/1000], Loss: 0.3576\n",
            "Epoch [667/1000], Loss: 0.3619\n",
            "Epoch [668/1000], Loss: 0.3478\n",
            "Epoch [669/1000], Loss: 0.3685\n",
            "Epoch [670/1000], Loss: 0.3404\n",
            "Epoch [671/1000], Loss: 0.3548\n",
            "Epoch [672/1000], Loss: 0.3573\n",
            "Epoch [673/1000], Loss: 0.3580\n",
            "Epoch [674/1000], Loss: 0.3368\n",
            "Epoch [675/1000], Loss: 0.3707\n",
            "Epoch [676/1000], Loss: 0.3300\n",
            "Epoch [677/1000], Loss: 0.3653\n",
            "Epoch [678/1000], Loss: 0.3307\n",
            "Epoch [679/1000], Loss: 0.3453\n",
            "Epoch [680/1000], Loss: 0.3560\n",
            "Epoch [681/1000], Loss: 0.3411\n",
            "Epoch [682/1000], Loss: 0.3415\n",
            "Epoch [683/1000], Loss: 0.3406\n",
            "Epoch [684/1000], Loss: 0.3349\n",
            "Epoch [685/1000], Loss: 0.3444\n",
            "Epoch [686/1000], Loss: 0.3688\n",
            "Epoch [687/1000], Loss: 0.3373\n",
            "Epoch [688/1000], Loss: 0.3511\n",
            "Epoch [689/1000], Loss: 0.3697\n",
            "Epoch [690/1000], Loss: 0.3361\n",
            "Epoch [691/1000], Loss: 0.3453\n",
            "Epoch [692/1000], Loss: 0.3687\n",
            "Epoch [693/1000], Loss: 0.3566\n",
            "Epoch [694/1000], Loss: 0.3504\n",
            "Epoch [695/1000], Loss: 0.3418\n",
            "Epoch [696/1000], Loss: 0.3355\n",
            "Epoch [697/1000], Loss: 0.3459\n",
            "Epoch [698/1000], Loss: 0.3430\n",
            "Epoch [699/1000], Loss: 0.3328\n",
            "Epoch [700/1000], Loss: 0.3599\n",
            "Epoch [701/1000], Loss: 0.3207\n",
            "Epoch [702/1000], Loss: 0.3594\n",
            "Epoch [703/1000], Loss: 0.3504\n",
            "Epoch [704/1000], Loss: 0.3546\n",
            "Epoch [705/1000], Loss: 0.3498\n",
            "Epoch [706/1000], Loss: 0.3377\n",
            "Epoch [707/1000], Loss: 0.3335\n",
            "Epoch [708/1000], Loss: 0.3524\n",
            "Epoch [709/1000], Loss: 0.3478\n",
            "Epoch [710/1000], Loss: 0.3483\n",
            "Epoch [711/1000], Loss: 0.3402\n",
            "Epoch [712/1000], Loss: 0.3380\n",
            "Epoch [713/1000], Loss: 0.3294\n",
            "Epoch [714/1000], Loss: 0.3644\n",
            "Epoch [715/1000], Loss: 0.3703\n",
            "Epoch [716/1000], Loss: 0.3594\n",
            "Epoch [717/1000], Loss: 0.3360\n",
            "Epoch [718/1000], Loss: 0.3372\n",
            "Epoch [719/1000], Loss: 0.3504\n",
            "Epoch [720/1000], Loss: 0.3718\n",
            "Epoch [721/1000], Loss: 0.3527\n",
            "Epoch [722/1000], Loss: 0.3515\n",
            "Epoch [723/1000], Loss: 0.3448\n",
            "Epoch [724/1000], Loss: 0.3679\n",
            "Epoch [725/1000], Loss: 0.3305\n",
            "Epoch [726/1000], Loss: 0.3647\n",
            "Epoch [727/1000], Loss: 0.3338\n",
            "Epoch [728/1000], Loss: 0.3657\n",
            "Epoch [729/1000], Loss: 0.3291\n",
            "Epoch [730/1000], Loss: 0.3552\n",
            "Epoch [731/1000], Loss: 0.3500\n",
            "Epoch [732/1000], Loss: 0.3421\n",
            "Epoch [733/1000], Loss: 0.3489\n",
            "Epoch [734/1000], Loss: 0.3391\n",
            "Epoch [735/1000], Loss: 0.3527\n",
            "Epoch [736/1000], Loss: 0.3975\n",
            "Epoch [737/1000], Loss: 0.3643\n",
            "Epoch [738/1000], Loss: 0.3502\n",
            "Epoch [739/1000], Loss: 0.3578\n",
            "Epoch [740/1000], Loss: 0.3268\n",
            "Epoch [741/1000], Loss: 0.3502\n",
            "Epoch [742/1000], Loss: 0.3512\n",
            "Epoch [743/1000], Loss: 0.3324\n",
            "Epoch [744/1000], Loss: 0.3615\n",
            "Epoch [745/1000], Loss: 0.3419\n",
            "Epoch [746/1000], Loss: 0.3173\n",
            "Epoch [747/1000], Loss: 0.3695\n",
            "Epoch [748/1000], Loss: 0.3538\n",
            "Epoch [749/1000], Loss: 0.3472\n",
            "Epoch [750/1000], Loss: 0.3445\n",
            "Epoch [751/1000], Loss: 0.3415\n",
            "Epoch [752/1000], Loss: 0.3514\n",
            "Epoch [753/1000], Loss: 0.3646\n",
            "Epoch [754/1000], Loss: 0.3710\n",
            "Epoch [755/1000], Loss: 0.3617\n",
            "Epoch [756/1000], Loss: 0.3518\n",
            "Epoch [757/1000], Loss: 0.3116\n",
            "Epoch [758/1000], Loss: 0.3687\n",
            "Epoch [759/1000], Loss: 0.3433\n",
            "Epoch [760/1000], Loss: 0.3482\n",
            "Epoch [761/1000], Loss: 0.3588\n",
            "Epoch [762/1000], Loss: 0.3363\n",
            "Epoch [763/1000], Loss: 0.3660\n",
            "Epoch [764/1000], Loss: 0.3249\n",
            "Epoch [765/1000], Loss: 0.3616\n",
            "Epoch [766/1000], Loss: 0.3742\n",
            "Epoch [767/1000], Loss: 0.3386\n",
            "Epoch [768/1000], Loss: 0.3463\n",
            "Epoch [769/1000], Loss: 0.3620\n",
            "Epoch [770/1000], Loss: 0.3401\n",
            "Epoch [771/1000], Loss: 0.3456\n",
            "Epoch [772/1000], Loss: 0.3445\n",
            "Epoch [773/1000], Loss: 0.3634\n",
            "Epoch [774/1000], Loss: 0.3560\n",
            "Epoch [775/1000], Loss: 0.3675\n",
            "Epoch [776/1000], Loss: 0.3564\n",
            "Epoch [777/1000], Loss: 0.3278\n",
            "Epoch [778/1000], Loss: 0.3425\n",
            "Epoch [779/1000], Loss: 0.3296\n",
            "Epoch [780/1000], Loss: 0.3342\n",
            "Epoch [781/1000], Loss: 0.3566\n",
            "Epoch [782/1000], Loss: 0.3613\n",
            "Epoch [783/1000], Loss: 0.3454\n",
            "Epoch [784/1000], Loss: 0.3568\n",
            "Epoch [785/1000], Loss: 0.3369\n",
            "Epoch [786/1000], Loss: 0.3219\n",
            "Epoch [787/1000], Loss: 0.3514\n",
            "Epoch [788/1000], Loss: 0.3641\n",
            "Epoch [789/1000], Loss: 0.3785\n",
            "Epoch [790/1000], Loss: 0.3389\n",
            "Epoch [791/1000], Loss: 0.3212\n",
            "Epoch [792/1000], Loss: 0.3416\n",
            "Epoch [793/1000], Loss: 0.3465\n",
            "Epoch [794/1000], Loss: 0.3559\n",
            "Epoch [795/1000], Loss: 0.3181\n",
            "Epoch [796/1000], Loss: 0.3415\n",
            "Epoch [797/1000], Loss: 0.3187\n",
            "Epoch [798/1000], Loss: 0.3580\n",
            "Epoch [799/1000], Loss: 0.3416\n",
            "Epoch [800/1000], Loss: 0.3734\n",
            "Epoch [801/1000], Loss: 0.3560\n",
            "Epoch [802/1000], Loss: 0.3180\n",
            "Epoch [803/1000], Loss: 0.3561\n",
            "Epoch [804/1000], Loss: 0.3723\n",
            "Epoch [805/1000], Loss: 0.3520\n",
            "Epoch [806/1000], Loss: 0.3429\n",
            "Epoch [807/1000], Loss: 0.3246\n",
            "Epoch [808/1000], Loss: 0.3347\n",
            "Epoch [809/1000], Loss: 0.3516\n",
            "Epoch [810/1000], Loss: 0.3269\n",
            "Epoch [811/1000], Loss: 0.3342\n",
            "Epoch [812/1000], Loss: 0.3493\n",
            "Epoch [813/1000], Loss: 0.3343\n",
            "Epoch [814/1000], Loss: 0.3373\n",
            "Epoch [815/1000], Loss: 0.3552\n",
            "Epoch [816/1000], Loss: 0.3379\n",
            "Epoch [817/1000], Loss: 0.3832\n",
            "Epoch [818/1000], Loss: 0.3258\n",
            "Epoch [819/1000], Loss: 0.3462\n",
            "Epoch [820/1000], Loss: 0.3844\n",
            "Epoch [821/1000], Loss: 0.3657\n",
            "Epoch [822/1000], Loss: 0.3713\n",
            "Epoch [823/1000], Loss: 0.3351\n",
            "Epoch [824/1000], Loss: 0.3685\n",
            "Epoch [825/1000], Loss: 0.3417\n",
            "Epoch [826/1000], Loss: 0.3503\n",
            "Epoch [827/1000], Loss: 0.3248\n",
            "Epoch [828/1000], Loss: 0.3693\n",
            "Epoch [829/1000], Loss: 0.3280\n",
            "Epoch [830/1000], Loss: 0.3650\n",
            "Epoch [831/1000], Loss: 0.3462\n",
            "Epoch [832/1000], Loss: 0.3551\n",
            "Epoch [833/1000], Loss: 0.3412\n",
            "Epoch [834/1000], Loss: 0.3449\n",
            "Epoch [835/1000], Loss: 0.3243\n",
            "Epoch [836/1000], Loss: 0.3330\n",
            "Epoch [837/1000], Loss: 0.3293\n",
            "Epoch [838/1000], Loss: 0.3655\n",
            "Epoch [839/1000], Loss: 0.3464\n",
            "Epoch [840/1000], Loss: 0.3324\n",
            "Epoch [841/1000], Loss: 0.3416\n",
            "Epoch [842/1000], Loss: 0.3351\n",
            "Epoch [843/1000], Loss: 0.3290\n",
            "Epoch [844/1000], Loss: 0.3416\n",
            "Epoch [845/1000], Loss: 0.3277\n",
            "Epoch [846/1000], Loss: 0.3265\n",
            "Epoch [847/1000], Loss: 0.3299\n",
            "Epoch [848/1000], Loss: 0.3590\n",
            "Epoch [849/1000], Loss: 0.3357\n",
            "Epoch [850/1000], Loss: 0.3244\n",
            "Epoch [851/1000], Loss: 0.3547\n",
            "Epoch [852/1000], Loss: 0.3496\n",
            "Epoch [853/1000], Loss: 0.3532\n",
            "Epoch [854/1000], Loss: 0.3395\n",
            "Epoch [855/1000], Loss: 0.3307\n",
            "Epoch [856/1000], Loss: 0.3514\n",
            "Epoch [857/1000], Loss: 0.3546\n",
            "Epoch [858/1000], Loss: 0.3448\n",
            "Epoch [859/1000], Loss: 0.3343\n",
            "Epoch [860/1000], Loss: 0.3360\n",
            "Epoch [861/1000], Loss: 0.3442\n",
            "Epoch [862/1000], Loss: 0.3339\n",
            "Epoch [863/1000], Loss: 0.3275\n",
            "Epoch [864/1000], Loss: 0.3203\n",
            "Epoch [865/1000], Loss: 0.3627\n",
            "Epoch [866/1000], Loss: 0.3387\n",
            "Epoch [867/1000], Loss: 0.3467\n",
            "Epoch [868/1000], Loss: 0.3309\n",
            "Epoch [869/1000], Loss: 0.3608\n",
            "Epoch [870/1000], Loss: 0.3434\n",
            "Epoch [871/1000], Loss: 0.3516\n",
            "Epoch [872/1000], Loss: 0.3649\n",
            "Epoch [873/1000], Loss: 0.3310\n",
            "Epoch [874/1000], Loss: 0.3439\n",
            "Epoch [875/1000], Loss: 0.3393\n",
            "Epoch [876/1000], Loss: 0.3438\n",
            "Epoch [877/1000], Loss: 0.3388\n",
            "Epoch [878/1000], Loss: 0.3238\n",
            "Epoch [879/1000], Loss: 0.3458\n",
            "Epoch [880/1000], Loss: 0.3428\n",
            "Epoch [881/1000], Loss: 0.3199\n",
            "Epoch [882/1000], Loss: 0.3365\n",
            "Epoch [883/1000], Loss: 0.3294\n",
            "Epoch [884/1000], Loss: 0.3431\n",
            "Epoch [885/1000], Loss: 0.3439\n",
            "Epoch [886/1000], Loss: 0.3432\n",
            "Epoch [887/1000], Loss: 0.3200\n",
            "Epoch [888/1000], Loss: 0.3270\n",
            "Epoch [889/1000], Loss: 0.3200\n",
            "Epoch [890/1000], Loss: 0.3282\n",
            "Epoch [891/1000], Loss: 0.3839\n",
            "Epoch [892/1000], Loss: 0.3262\n",
            "Epoch [893/1000], Loss: 0.3432\n",
            "Epoch [894/1000], Loss: 0.3385\n",
            "Epoch [895/1000], Loss: 0.3408\n",
            "Epoch [896/1000], Loss: 0.3369\n",
            "Epoch [897/1000], Loss: 0.3413\n",
            "Epoch [898/1000], Loss: 0.3615\n",
            "Epoch [899/1000], Loss: 0.3575\n",
            "Epoch [900/1000], Loss: 0.3788\n",
            "Epoch [901/1000], Loss: 0.3538\n",
            "Epoch [902/1000], Loss: 0.3313\n",
            "Epoch [903/1000], Loss: 0.3613\n",
            "Epoch [904/1000], Loss: 0.3401\n",
            "Epoch [905/1000], Loss: 0.3285\n",
            "Epoch [906/1000], Loss: 0.3293\n",
            "Epoch [907/1000], Loss: 0.3363\n",
            "Epoch [908/1000], Loss: 0.3449\n",
            "Epoch [909/1000], Loss: 0.3252\n",
            "Epoch [910/1000], Loss: 0.3295\n",
            "Epoch [911/1000], Loss: 0.3611\n",
            "Epoch [912/1000], Loss: 0.3404\n",
            "Epoch [913/1000], Loss: 0.3441\n",
            "Epoch [914/1000], Loss: 0.3476\n",
            "Epoch [915/1000], Loss: 0.3411\n",
            "Epoch [916/1000], Loss: 0.3200\n",
            "Epoch [917/1000], Loss: 0.3419\n",
            "Epoch [918/1000], Loss: 0.3427\n",
            "Epoch [919/1000], Loss: 0.3402\n",
            "Epoch [920/1000], Loss: 0.3298\n",
            "Epoch [921/1000], Loss: 0.3451\n",
            "Epoch [922/1000], Loss: 0.3552\n",
            "Epoch [923/1000], Loss: 0.3428\n",
            "Epoch [924/1000], Loss: 0.3386\n",
            "Epoch [925/1000], Loss: 0.3577\n",
            "Epoch [926/1000], Loss: 0.3397\n",
            "Epoch [927/1000], Loss: 0.3552\n",
            "Epoch [928/1000], Loss: 0.3333\n",
            "Epoch [929/1000], Loss: 0.3363\n",
            "Epoch [930/1000], Loss: 0.3376\n",
            "Epoch [931/1000], Loss: 0.3347\n",
            "Epoch [932/1000], Loss: 0.3447\n",
            "Epoch [933/1000], Loss: 0.3422\n",
            "Epoch [934/1000], Loss: 0.3376\n",
            "Epoch [935/1000], Loss: 0.3701\n",
            "Epoch [936/1000], Loss: 0.3407\n",
            "Epoch [937/1000], Loss: 0.3437\n",
            "Epoch [938/1000], Loss: 0.3452\n",
            "Epoch [939/1000], Loss: 0.3207\n",
            "Epoch [940/1000], Loss: 0.3339\n",
            "Epoch [941/1000], Loss: 0.3345\n",
            "Epoch [942/1000], Loss: 0.3337\n",
            "Epoch [943/1000], Loss: 0.3563\n",
            "Epoch [944/1000], Loss: 0.3353\n",
            "Epoch [945/1000], Loss: 0.3312\n",
            "Epoch [946/1000], Loss: 0.3249\n",
            "Epoch [947/1000], Loss: 0.3331\n",
            "Epoch [948/1000], Loss: 0.3747\n",
            "Epoch [949/1000], Loss: 0.3386\n",
            "Epoch [950/1000], Loss: 0.3603\n",
            "Epoch [951/1000], Loss: 0.3271\n",
            "Epoch [952/1000], Loss: 0.3266\n",
            "Epoch [953/1000], Loss: 0.3391\n",
            "Epoch [954/1000], Loss: 0.3348\n",
            "Epoch [955/1000], Loss: 0.3416\n",
            "Epoch [956/1000], Loss: 0.3287\n",
            "Epoch [957/1000], Loss: 0.3494\n",
            "Epoch [958/1000], Loss: 0.3377\n",
            "Epoch [959/1000], Loss: 0.3119\n",
            "Epoch [960/1000], Loss: 0.3463\n",
            "Epoch [961/1000], Loss: 0.3316\n",
            "Epoch [962/1000], Loss: 0.3375\n",
            "Epoch [963/1000], Loss: 0.3278\n",
            "Epoch [964/1000], Loss: 0.3651\n",
            "Epoch [965/1000], Loss: 0.3271\n",
            "Epoch [966/1000], Loss: 0.3394\n",
            "Epoch [967/1000], Loss: 0.3412\n",
            "Epoch [968/1000], Loss: 0.3508\n",
            "Epoch [969/1000], Loss: 0.3455\n",
            "Epoch [970/1000], Loss: 0.3296\n",
            "Epoch [971/1000], Loss: 0.3419\n",
            "Epoch [972/1000], Loss: 0.3363\n",
            "Epoch [973/1000], Loss: 0.3486\n",
            "Epoch [974/1000], Loss: 0.3312\n",
            "Epoch [975/1000], Loss: 0.3282\n",
            "Epoch [976/1000], Loss: 0.3435\n",
            "Epoch [977/1000], Loss: 0.3696\n",
            "Epoch [978/1000], Loss: 0.3240\n",
            "Epoch [979/1000], Loss: 0.3342\n",
            "Epoch [980/1000], Loss: 0.3455\n",
            "Epoch [981/1000], Loss: 0.3440\n",
            "Epoch [982/1000], Loss: 0.3524\n",
            "Epoch [983/1000], Loss: 0.3319\n",
            "Epoch [984/1000], Loss: 0.3251\n",
            "Epoch [985/1000], Loss: 0.3610\n",
            "Epoch [986/1000], Loss: 0.3386\n",
            "Epoch [987/1000], Loss: 0.3529\n",
            "Epoch [988/1000], Loss: 0.3381\n",
            "Epoch [989/1000], Loss: 0.3252\n",
            "Epoch [990/1000], Loss: 0.3224\n",
            "Epoch [991/1000], Loss: 0.3592\n",
            "Epoch [992/1000], Loss: 0.3245\n",
            "Epoch [993/1000], Loss: 0.3383\n",
            "Epoch [994/1000], Loss: 0.3541\n",
            "Epoch [995/1000], Loss: 0.3287\n",
            "Epoch [996/1000], Loss: 0.3278\n",
            "Epoch [997/1000], Loss: 0.3352\n",
            "Epoch [998/1000], Loss: 0.3386\n",
            "Epoch [999/1000], Loss: 0.3323\n",
            "Epoch [1000/1000], Loss: 0.3324\n",
            "Accuracy on test set: 0.8601\n"
          ]
        }
      ]
    }
  ]
}